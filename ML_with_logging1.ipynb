{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ce73e07",
   "metadata": {},
   "source": [
    "# Enhanced MLP Architecture + Hyperparameter Exploration Notebook\n",
    "\n",
    "**What this notebook does**\n",
    "- Loads a CSV file that is already *categorically encoded* (user must provide path).\n",
    "- Tries a configurable range of MLP architectures and hyperparameters (hidden layers, solvers, batch sizes, learning rates, epochs, and — for PyTorch models — dropout).\n",
    "- Runs experiments using both **scikit-learn MLP** (fast, good for many grid search experiments) and an optional **PyTorch MLP** (to test dropout explicitly and obtain training loss/accuracy curves).\n",
    "- **Enhanced logging**: Comprehensive logging with TensorBoard, MLflow, progress tracking, error handling, and resource monitoring.\n",
    "- Saves several graphs (PNG files) showing how different parameters affect training/test performance and training loss curves.\n",
    "- Saves results to a CSV for later inspection.\n",
    "\n",
    "**Notes**\n",
    "- The notebook assumes the CSV includes features and a target column (default target column is `risk_level`). If your target has string labels, the notebook will attempt to map `low/medium/high` to `0/1/2`. Adjust the mapping if needed.\n",
    "- The notebook uses `StandardScaler` to scale inputs (recommended for neural nets).\n",
    "- The PyTorch implementation supports dropout and logs all metrics to TensorBoard and optionally MLflow.\n",
    "- Enhanced with comprehensive logging, error handling, and resource monitoring.\n",
    "\n",
    "You can run this notebook end-to-end; change the parameter grids near the top to expand or narrow the search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "setup_logging",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 00:12:30,876 - INFO - Starting ML experiments at 2025-09-06 00:12:30.876181\n",
      "2025-09-06 00:12:30,877 - INFO - Output directory: experiment_outputs\n",
      "2025-09-06 00:12:30,877 - INFO - PyTorch enabled: True\n",
      "2025-09-06 00:12:30,878 - INFO - MLflow enabled: True\n",
      "2025-09-06 00:12:30,878 - INFO - Cross-validation enabled: True (5 folds)\n",
      "2025-09-06 00:12:30,877 - INFO - Output directory: experiment_outputs\n",
      "2025-09-06 00:12:30,877 - INFO - PyTorch enabled: True\n",
      "2025-09-06 00:12:30,878 - INFO - MLflow enabled: True\n",
      "2025-09-06 00:12:30,878 - INFO - Cross-validation enabled: True (5 folds)\n"
     ]
    }
   ],
   "source": [
    "# Enhanced imports with logging and monitoring\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import product\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import traceback\n",
    "import psutil\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "# ML imports\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (accuracy_score, classification_report, confusion_matrix, \n",
    "                           precision_score, recall_score, f1_score, precision_recall_fscore_support)\n",
    "import joblib  # For saving/loading models and data\n",
    "\n",
    "# Set up enhanced logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('experiment.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration\n",
    "OUTDIR = 'experiment_outputs'\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTDIR, 'models'), exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTDIR, 'tb_logs'), exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTDIR, 'sklearn_models'), exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTDIR, 'pytorch_models'), exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTDIR, 'metrics'), exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTDIR, 'confusion_matrices'), exist_ok=True)\n",
    "\n",
    "# Experiment configuration\n",
    "use_pytorch = True  # Set to False to skip PyTorch experiments\n",
    "use_mlflow = True  # Set to True to enable MLflow logging\n",
    "use_cross_validation = True  # Set to True to enable 5-fold cross-validation\n",
    "n_folds = 5  # Number of cross-validation folds\n",
    "data_path = '/mnt/data/sample.csv'  # Change this to your CSV path\n",
    "\n",
    "logger.info(f\"Starting ML experiments at {datetime.now()}\")\n",
    "logger.info(f\"Output directory: {OUTDIR}\")\n",
    "logger.info(f\"PyTorch enabled: {use_pytorch}\")\n",
    "logger.info(f\"MLflow enabled: {use_mlflow}\")\n",
    "logger.info(f\"Cross-validation enabled: {use_cross_validation} ({n_folds} folds)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "data_loading",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 00:02:00,157 - INFO - Loading data from /mnt/data/sample.csv\n",
      "2025-09-06 00:02:00,211 - INFO - Data loaded successfully. Shape: (25000, 69)\n",
      "2025-09-06 00:02:00,211 - INFO - Columns: ['has_dental_data', 'has_dietary_data', 'sa_citizen', 'special_needs', 'caregiver_treatment', 'appliance', 'plaque', 'dry_mouth', 'enamel_defects', 'fluoride_water', 'fluoride_toothpaste', 'topical_fluoride', 'regular_checkups', 'sealed_pits', 'restorative_procedures', 'enamel_change', 'dentin_discoloration', 'white_spot_lesions', 'cavitated_lesions', 'multiple_restorations', 'missing_teeth', 'total_dmft_score', 'sweet_sugary_foods', 'sweet_sugary_foods_bedtime', 'sweet_sugary_foods_daily', 'sweet_sugary_foods_weekly', 'sweet_sugary_foods_timing', 'takeaways_processed_foods', 'takeaways_processed_foods_daily', 'takeaways_processed_foods_weekly', 'fresh_fruit', 'fresh_fruit_bedtime', 'fresh_fruit_daily', 'fresh_fruit_weekly', 'fresh_fruit_timing', 'cold_drinks_juices', 'cold_drinks_juices_bedtime', 'cold_drinks_juices_daily', 'cold_drinks_juices_weekly', 'cold_drinks_juices_timing', 'processed_fruit', 'processed_fruit_bedtime', 'processed_fruit_daily', 'processed_fruit_weekly', 'processed_fruit_timing', 'spreads', 'spreads_bedtime', 'spreads_daily', 'spreads_weekly', 'spreads_timing', 'added_sugars', 'added_sugars_bedtime', 'added_sugars_daily', 'added_sugars_weekly', 'added_sugars_timing', 'salty_snacks', 'salty_snacks_daily', 'salty_snacks_weekly', 'salty_snacks_timing', 'dairy_products', 'dairy_products_daily', 'dairy_products_weekly', 'vegetables', 'vegetables_daily', 'vegetables_weekly', 'water', 'water_timing', 'water_glasses', 'risk_level']\n",
      "2025-09-06 00:02:00,219 - INFO - Converting categorical target. Unique values: ['medium' 'high' 'low']\n",
      "2025-09-06 00:02:00,211 - INFO - Data loaded successfully. Shape: (25000, 69)\n",
      "2025-09-06 00:02:00,211 - INFO - Columns: ['has_dental_data', 'has_dietary_data', 'sa_citizen', 'special_needs', 'caregiver_treatment', 'appliance', 'plaque', 'dry_mouth', 'enamel_defects', 'fluoride_water', 'fluoride_toothpaste', 'topical_fluoride', 'regular_checkups', 'sealed_pits', 'restorative_procedures', 'enamel_change', 'dentin_discoloration', 'white_spot_lesions', 'cavitated_lesions', 'multiple_restorations', 'missing_teeth', 'total_dmft_score', 'sweet_sugary_foods', 'sweet_sugary_foods_bedtime', 'sweet_sugary_foods_daily', 'sweet_sugary_foods_weekly', 'sweet_sugary_foods_timing', 'takeaways_processed_foods', 'takeaways_processed_foods_daily', 'takeaways_processed_foods_weekly', 'fresh_fruit', 'fresh_fruit_bedtime', 'fresh_fruit_daily', 'fresh_fruit_weekly', 'fresh_fruit_timing', 'cold_drinks_juices', 'cold_drinks_juices_bedtime', 'cold_drinks_juices_daily', 'cold_drinks_juices_weekly', 'cold_drinks_juices_timing', 'processed_fruit', 'processed_fruit_bedtime', 'processed_fruit_daily', 'processed_fruit_weekly', 'processed_fruit_timing', 'spreads', 'spreads_bedtime', 'spreads_daily', 'spreads_weekly', 'spreads_timing', 'added_sugars', 'added_sugars_bedtime', 'added_sugars_daily', 'added_sugars_weekly', 'added_sugars_timing', 'salty_snacks', 'salty_snacks_daily', 'salty_snacks_weekly', 'salty_snacks_timing', 'dairy_products', 'dairy_products_daily', 'dairy_products_weekly', 'vegetables', 'vegetables_daily', 'vegetables_weekly', 'water', 'water_timing', 'water_glasses', 'risk_level']\n",
      "2025-09-06 00:02:00,219 - INFO - Converting categorical target. Unique values: ['medium' 'high' 'low']\n",
      "2025-09-06 00:02:00,222 - INFO - Mapped risk levels: low->0, medium->1, high->2\n",
      "2025-09-06 00:02:00,223 - INFO - Target distribution: [ 4522  9247 11231]\n",
      "2025-09-06 00:02:00,222 - INFO - Mapped risk levels: low->0, medium->1, high->2\n",
      "2025-09-06 00:02:00,223 - INFO - Target distribution: [ 4522  9247 11231]\n",
      "2025-09-06 00:02:00,240 - INFO - Train-test split completed. Train: (20000, 68), Test: (5000, 68)\n",
      "2025-09-06 00:02:00,240 - INFO - Train-test split completed. Train: (20000, 68), Test: (5000, 68)\n",
      "2025-09-06 00:02:00,261 - INFO - Features scaled using StandardScaler\n",
      "2025-09-06 00:02:00,261 - INFO - Features scaled using StandardScaler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Info:\n",
      "Shape: (25000, 69)\n",
      "Columns: ['has_dental_data', 'has_dietary_data', 'sa_citizen', 'special_needs', 'caregiver_treatment', 'appliance', 'plaque', 'dry_mouth', 'enamel_defects', 'fluoride_water', 'fluoride_toothpaste', 'topical_fluoride', 'regular_checkups', 'sealed_pits', 'restorative_procedures', 'enamel_change', 'dentin_discoloration', 'white_spot_lesions', 'cavitated_lesions', 'multiple_restorations', 'missing_teeth', 'total_dmft_score', 'sweet_sugary_foods', 'sweet_sugary_foods_bedtime', 'sweet_sugary_foods_daily', 'sweet_sugary_foods_weekly', 'sweet_sugary_foods_timing', 'takeaways_processed_foods', 'takeaways_processed_foods_daily', 'takeaways_processed_foods_weekly', 'fresh_fruit', 'fresh_fruit_bedtime', 'fresh_fruit_daily', 'fresh_fruit_weekly', 'fresh_fruit_timing', 'cold_drinks_juices', 'cold_drinks_juices_bedtime', 'cold_drinks_juices_daily', 'cold_drinks_juices_weekly', 'cold_drinks_juices_timing', 'processed_fruit', 'processed_fruit_bedtime', 'processed_fruit_daily', 'processed_fruit_weekly', 'processed_fruit_timing', 'spreads', 'spreads_bedtime', 'spreads_daily', 'spreads_weekly', 'spreads_timing', 'added_sugars', 'added_sugars_bedtime', 'added_sugars_daily', 'added_sugars_weekly', 'added_sugars_timing', 'salty_snacks', 'salty_snacks_daily', 'salty_snacks_weekly', 'salty_snacks_timing', 'dairy_products', 'dairy_products_daily', 'dairy_products_weekly', 'vegetables', 'vegetables_daily', 'vegetables_weekly', 'water', 'water_timing', 'water_glasses', 'risk_level']\n",
      "Missing values: 0\n"
     ]
    }
   ],
   "source": [
    "# Helper function for comprehensive metrics calculation\n",
    "def calculate_comprehensive_metrics(y_true, y_pred, model_name, experiment_id, model_type='sklearn'):\n",
    "    \"\"\"Calculate and save comprehensive metrics for a model\"\"\"\n",
    "    \n",
    "    # Calculate all metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision_macro = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    recall_macro = recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    f1_macro = f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    \n",
    "    precision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    recall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    f1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    # Per-class metrics\n",
    "    precision_per_class, recall_per_class, f1_per_class, support = precision_recall_fscore_support(\n",
    "        y_true, y_pred, zero_division=0)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Create comprehensive metrics dictionary\n",
    "    metrics = {\n",
    "        'model_name': model_name,\n",
    "        'experiment_id': experiment_id,\n",
    "        'model_type': model_type,\n",
    "        'accuracy': accuracy,\n",
    "        'precision_macro': precision_macro,\n",
    "        'recall_macro': recall_macro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'precision_weighted': precision_weighted,\n",
    "        'recall_weighted': recall_weighted,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'confusion_matrix': cm.tolist(),\n",
    "        'per_class_metrics': {\n",
    "            'precision': precision_per_class.tolist(),\n",
    "            'recall': recall_per_class.tolist(),\n",
    "            'f1_score': f1_per_class.tolist(),\n",
    "            'support': support.tolist()\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save metrics to JSON\n",
    "    metrics_file = os.path.join(OUTDIR, 'metrics', f'{model_type}_model_{experiment_id}_metrics.json')\n",
    "    with open(metrics_file, 'w') as f:\n",
    "        json.dump(metrics, f, indent=2)\n",
    "    \n",
    "    # Create and save confusion matrix plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=[f'Class {i}' for i in range(cm.shape[1])],\n",
    "                yticklabels=[f'Class {i}' for i in range(cm.shape[0])])\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    \n",
    "    cm_file = os.path.join(OUTDIR, 'confusion_matrices', f'{model_type}_model_{experiment_id}_confusion_matrix.png')\n",
    "    plt.savefig(cm_file, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Generate classification report\n",
    "    class_report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
    "    report_file = os.path.join(OUTDIR, 'metrics', f'{model_type}_model_{experiment_id}_classification_report.json')\n",
    "    with open(report_file, 'w') as f:\n",
    "        json.dump(class_report, f, indent=2)\n",
    "    \n",
    "    logger.info(f\"Comprehensive metrics saved for {model_name} (ID: {experiment_id})\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Load and prepare data\n",
    "try:\n",
    "    logger.info(f\"Loading data from {data_path}\")\n",
    "    df = pd.read_csv(\"training_data.csv\")\n",
    "    logger.info(f\"Data loaded successfully. Shape: {df.shape}\")\n",
    "    logger.info(f\"Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Display basic info about the dataset\n",
    "    print(\"Dataset Info:\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    print(f\"Missing values: {df.isnull().sum().sum()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load data: {str(e)}\")\n",
    "    logger.error(traceback.format_exc())\n",
    "    raise\n",
    "\n",
    "# Prepare features and target\n",
    "target_col = 'risk_level'  # Change this if your target column has a different name\n",
    "\n",
    "try:\n",
    "    if target_col not in df.columns:\n",
    "        logger.warning(f\"Target column '{target_col}' not found. Available columns: {list(df.columns)}\")\n",
    "        target_col = input(\"Please enter the correct target column name: \")\n",
    "    \n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Handle categorical target if needed\n",
    "    if y.dtype == 'object':\n",
    "        logger.info(f\"Converting categorical target. Unique values: {y.unique()}\")\n",
    "        if set(y.unique()).issubset({'low', 'medium', 'high'}):\n",
    "            y = y.map({'low': 0, 'medium': 1, 'high': 2})\n",
    "            logger.info(\"Mapped risk levels: low->0, medium->1, high->2\")\n",
    "        else:\n",
    "            # Use label encoding for other categorical targets\n",
    "            from sklearn.preprocessing import LabelEncoder\n",
    "            le = LabelEncoder()\n",
    "            y = le.fit_transform(y)\n",
    "            logger.info(f\"Label encoded target. Classes: {le.classes_}\")\n",
    "    \n",
    "    logger.info(f\"Target distribution: {np.bincount(y)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error preparing target variable: {str(e)}\")\n",
    "    logger.error(traceback.format_exc())\n",
    "    raise\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "logger.info(f\"Train-test split completed. Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "logger.info(\"Features scaled using StandardScaler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b97b9e52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU device: NVIDIA GeForce RTX 4050 Laptop GPU\n",
      "GPU memory: 6.0 GB\n",
      "\n",
      "Note: Scikit-learn experiments always use CPU only\n",
      "PyTorch experiments will use GPU\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "try:\n",
    "    import torch\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    print(f\"CUDA available: {gpu_available}\")\n",
    "    if gpu_available:\n",
    "        print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "    else:\n",
    "        print(\"No GPU available - will use CPU for PyTorch experiments\")\n",
    "except ImportError:\n",
    "    print(\"PyTorch not installed - only scikit-learn experiments will run (CPU only)\")\n",
    "\n",
    "print(f\"\\nNote: Scikit-learn experiments always use CPU only\")\n",
    "print(f\"PyTorch experiments will use {'GPU' if gpu_available else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889eaa61",
   "metadata": {},
   "source": [
    "## PyTorch GPU Installation\n",
    "\n",
    "Based on your NVIDIA GeForce RTX 4050 with CUDA 12.9, install PyTorch with GPU support using one of these commands:\n",
    "\n",
    "### Option 1: Latest PyTorch with CUDA 12.1 (Recommended)\n",
    "```bash\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "```\n",
    "\n",
    "### Option 2: Latest PyTorch with CUDA 11.8 (Alternative)\n",
    "```bash\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "```\n",
    "\n",
    "### Option 3: CPU-only version (if GPU setup fails)\n",
    "```bash\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "```\n",
    "\n",
    "**Note:** The CUDA 12.1 version should work with your CUDA 12.9 driver (backwards compatible).\n",
    "\n",
    "Run the cell below to check if GPU is detected after installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "experiment_config",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 00:02:02,606 - INFO - Experiment configuration saved to experiment_outputs\\experiment_config.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total sklearn combinations: 1\n",
      "Total PyTorch combinations: 4\n"
     ]
    }
   ],
   "source": [
    "# Parameter grids\n",
    "\n",
    "# sklearn_param_grid = {\n",
    "#     'hidden_layer_sizes': [(25,), (32,), (64,)],\n",
    "#     'solver': ['adam', 'lbfgs', 'adamw'],\n",
    "#     'learning_rate_init': [0.01, 0.1],\n",
    "#     'max_iter': [200, 500, 1000]\n",
    "# }\n",
    "\n",
    "sklearn_param_grid = {\n",
    "    'hidden_layer_sizes': [(64,)],\n",
    "    'solver': ['adamw'],\n",
    "    'learning_rate_init': [0.01],\n",
    "    'max_iter': [500]\n",
    "}\n",
    "\n",
    "# pytorch_param_grid = {\n",
    "#     'hidden_sizes': [[25], [32], [64]],\n",
    "#     'learning_rate': [0.001, 0.01, 0.1],\n",
    "#     'dropout': [0.2, 0.5],\n",
    "#     'batch_size': [64, 128],\n",
    "#     'epochs': [100, 150]\n",
    "# }\n",
    "\n",
    "pytorch_param_grid = {\n",
    "    'hidden_sizes': [[64]],\n",
    "    'learning_rate': [0.001],\n",
    "    'dropout': [0.2],\n",
    "    'batch_size': [64, 128],\n",
    "    'epochs': [100, 150]\n",
    "}\n",
    "\n",
    "# Save experiment configuration\n",
    "experiment_config = {\n",
    "    'sklearn_param_grid': sklearn_param_grid,\n",
    "    'pytorch_param_grid': pytorch_param_grid,\n",
    "    'data_path': data_path,\n",
    "    'target_column': target_col,\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'train_size': X_train.shape[0],\n",
    "    'test_size': X_test.shape[0],\n",
    "    'n_features': X_train.shape[1],\n",
    "    'n_classes': len(np.unique(y)),\n",
    "    'use_pytorch': use_pytorch,\n",
    "    'use_mlflow': use_mlflow\n",
    "}\n",
    "\n",
    "with open(os.path.join(OUTDIR, 'experiment_config.yaml'), 'w') as f:\n",
    "    yaml.dump(experiment_config, f, default_flow_style=False)\n",
    "\n",
    "logger.info(f\"Experiment configuration saved to {os.path.join(OUTDIR, 'experiment_config.yaml')}\")\n",
    "print(f\"Total sklearn combinations: {len(list(product(*sklearn_param_grid.values())))}\")\n",
    "if use_pytorch:\n",
    "    print(f\"Total PyTorch combinations: {len(list(product(*pytorch_param_grid.values())))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1aabc9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 00:02:02,615 - INFO - Selected 35 features for training\n",
      "2025-09-06 00:02:02,616 - INFO - Selected features: ['has_dental_data', 'special_needs', 'caregiver_treatment', 'plaque', 'dry_mouth', 'enamel_defects', 'fluoride_water', 'fluoride_toothpaste', 'topical_fluoride', 'regular_checkups', 'sealed_pits', 'enamel_change', 'dentin_discoloration', 'white_spot_lesions', 'cavitated_lesions', 'multiple_restorations', 'missing_teeth', 'total_dmft_score', 'sweet_sugary_foods', 'sweet_sugary_foods_bedtime', 'takeaways_processed_foods', 'fresh_fruit', 'cold_drinks_juices', 'processed_fruit', 'processed_fruit_bedtime', 'spreads', 'spreads_bedtime', 'added_sugars', 'added_sugars_bedtime', 'salty_snacks', 'dairy_products', 'vegetables', 'has_dietary_data', 'cold_drinks_juices_bedtime', 'water']\n",
      "2025-09-06 00:02:02,616 - INFO - Available features: 35/35\n",
      "2025-09-06 00:02:02,621 - INFO - Feature matrix shape after selection: (25000, 35)\n",
      "2025-09-06 00:02:02,616 - INFO - Selected features: ['has_dental_data', 'special_needs', 'caregiver_treatment', 'plaque', 'dry_mouth', 'enamel_defects', 'fluoride_water', 'fluoride_toothpaste', 'topical_fluoride', 'regular_checkups', 'sealed_pits', 'enamel_change', 'dentin_discoloration', 'white_spot_lesions', 'cavitated_lesions', 'multiple_restorations', 'missing_teeth', 'total_dmft_score', 'sweet_sugary_foods', 'sweet_sugary_foods_bedtime', 'takeaways_processed_foods', 'fresh_fruit', 'cold_drinks_juices', 'processed_fruit', 'processed_fruit_bedtime', 'spreads', 'spreads_bedtime', 'added_sugars', 'added_sugars_bedtime', 'salty_snacks', 'dairy_products', 'vegetables', 'has_dietary_data', 'cold_drinks_juices_bedtime', 'water']\n",
      "2025-09-06 00:02:02,616 - INFO - Available features: 35/35\n",
      "2025-09-06 00:02:02,621 - INFO - Feature matrix shape after selection: (25000, 35)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features selected: 35\n",
      "Final feature set: ['has_dental_data', 'special_needs', 'caregiver_treatment', 'plaque', 'dry_mouth', 'enamel_defects', 'fluoride_water', 'fluoride_toothpaste', 'topical_fluoride', 'regular_checkups', 'sealed_pits', 'enamel_change', 'dentin_discoloration', 'white_spot_lesions', 'cavitated_lesions', 'multiple_restorations', 'missing_teeth', 'total_dmft_score', 'sweet_sugary_foods', 'sweet_sugary_foods_bedtime', 'takeaways_processed_foods', 'fresh_fruit', 'cold_drinks_juices', 'processed_fruit', 'processed_fruit_bedtime', 'spreads', 'spreads_bedtime', 'added_sugars', 'added_sugars_bedtime', 'salty_snacks', 'dairy_products', 'vegetables', 'has_dietary_data', 'cold_drinks_juices_bedtime', 'water']\n"
     ]
    }
   ],
   "source": [
    "# Feature Selection - Use only the selected features from feature selection analysis\n",
    "selected_features = [\n",
    "    'has_dental_data',\n",
    "    'special_needs',\n",
    "    'caregiver_treatment',\n",
    "    'plaque',\n",
    "    'dry_mouth',\n",
    "    'enamel_defects',\n",
    "    'fluoride_water',\n",
    "    'fluoride_toothpaste',\n",
    "    'topical_fluoride',\n",
    "    'regular_checkups',\n",
    "    'sealed_pits',\n",
    "    'enamel_change',\n",
    "    'dentin_discoloration',\n",
    "    'white_spot_lesions',\n",
    "    'cavitated_lesions',\n",
    "    'multiple_restorations',\n",
    "    'missing_teeth',\n",
    "    'total_dmft_score',\n",
    "    'sweet_sugary_foods',\n",
    "    'sweet_sugary_foods_bedtime',\n",
    "    'takeaways_processed_foods',\n",
    "    'fresh_fruit',\n",
    "    'cold_drinks_juices',\n",
    "    'processed_fruit',\n",
    "    'processed_fruit_bedtime',\n",
    "    'spreads',\n",
    "    'spreads_bedtime',\n",
    "    'added_sugars',\n",
    "    'added_sugars_bedtime',\n",
    "    'salty_snacks',\n",
    "    'dairy_products',\n",
    "    'vegetables',\n",
    "    'has_dietary_data',\n",
    "    'cold_drinks_juices_bedtime',\n",
    "    'water'\n",
    "]\n",
    "\n",
    "logger.info(f\"Selected {len(selected_features)} features for training\")\n",
    "logger.info(f\"Selected features: {selected_features}\")\n",
    "\n",
    "# Check which selected features are available in the dataset\n",
    "available_features = []\n",
    "missing_features = []\n",
    "\n",
    "for feature in selected_features:\n",
    "    if feature in X.columns:\n",
    "        available_features.append(feature)\n",
    "    else:\n",
    "        missing_features.append(feature)\n",
    "\n",
    "logger.info(f\"Available features: {len(available_features)}/{len(selected_features)}\")\n",
    "if missing_features:\n",
    "    logger.warning(f\"Missing features: {missing_features}\")\n",
    "\n",
    "# Filter X to only include selected features that are available\n",
    "X_selected = X[available_features].copy()\n",
    "logger.info(f\"Feature matrix shape after selection: {X_selected.shape}\")\n",
    "\n",
    "# Update X to use only selected features\n",
    "X = X_selected\n",
    "\n",
    "print(f\"Features selected: {len(available_features)}\")\n",
    "print(f\"Final feature set: {list(X.columns)}\")\n",
    "if missing_features:\n",
    "    print(f\"Warning: {len(missing_features)} features not found in dataset: {missing_features}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sklearn_experiments",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 00:02:02,641 - INFO - Starting 1 scikit-learn experiments\n",
      "2025-09-06 00:02:02,642 - INFO - Using 5-fold cross-validation (this will increase training time by ~5x)\n",
      "Scikit-learn experiments:   0%|          | 0/1 [00:00<?, ?it/s]2025-09-06 00:02:02,642 - INFO - Using 5-fold cross-validation (this will increase training time by ~5x)\n",
      "Scikit-learn experiments:   0%|          | 0/1 [00:00<?, ?it/s]2025-09-06 00:02:03,696 - ERROR - Sklearn experiment 1 failed with params {'hidden_layer_sizes': (64,), 'solver': 'adamw', 'learning_rate_init': 0.01, 'max_iter': 500}: \n",
      "All the 5 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\vhuta\\Desktop\\OralSmart - Data Analysis\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vhuta\\Desktop\\OralSmart - Data Analysis\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1358, in wrapper\n",
      "    estimator._validate_params()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"c:\\Users\\vhuta\\Desktop\\OralSmart - Data Analysis\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 471, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self._parameter_constraints,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.get_params(deep=False),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        caller_name=self.__class__.__name__,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\vhuta\\Desktop\\OralSmart - Data Analysis\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'solver' parameter of MLPClassifier must be a str among {'adam', 'sgd', 'lbfgs'}. Got 'adamw' instead.\n",
      "\n",
      "2025-09-06 00:02:03,698 - ERROR - Traceback (most recent call last):\n",
      "  File \"C:\\Users\\vhuta\\AppData\\Local\\Temp\\ipykernel_32588\\963565606.py\", line 36, in <module>\n",
      "    cv_scores = cross_val_score(clf, X_full, y_full, cv=cv, scoring='accuracy')\n",
      "  File \"c:\\Users\\vhuta\\Desktop\\OralSmart - Data Analysis\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 218, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\vhuta\\Desktop\\OralSmart - Data Analysis\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 677, in cross_val_score\n",
      "    cv_results = cross_validate(\n",
      "        estimator=estimator,\n",
      "    ...<9 lines>...\n",
      "        error_score=error_score,\n",
      "    )\n",
      "  File \"c:\\Users\\vhuta\\Desktop\\OralSmart - Data Analysis\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 218, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\vhuta\\Desktop\\OralSmart - Data Analysis\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 419, in cross_validate\n",
      "    _warn_or_raise_about_fit_failures(results, error_score)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vhuta\\Desktop\\OralSmart - Data Analysis\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 505, in _warn_or_raise_about_fit_failures\n",
      "    raise ValueError(all_fits_failed_message)\n",
      "ValueError: \n",
      "All the 5 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\vhuta\\Desktop\\OralSmart - Data Analysis\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vhuta\\Desktop\\OralSmart - Data Analysis\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1358, in wrapper\n",
      "    estimator._validate_params()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"c:\\Users\\vhuta\\Desktop\\OralSmart - Data Analysis\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 471, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self._parameter_constraints,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.get_params(deep=False),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        caller_name=self.__class__.__name__,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\vhuta\\Desktop\\OralSmart - Data Analysis\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'solver' parameter of MLPClassifier must be a str among {'adam', 'sgd', 'lbfgs'}. Got 'adamw' instead.\n",
      "\n",
      "\n",
      "Scikit-learn experiments: 100%|██████████| 1/1 [00:01<00:00,  1.05s/it]\n",
      "2025-09-06 00:02:03,699 - INFO - Completed 0 successful scikit-learn experiments\n",
      "2025-09-06 00:02:03,696 - ERROR - Sklearn experiment 1 failed with params {'hidden_layer_sizes': (64,), 'solver': 'adamw', 'learning_rate_init': 0.01, 'max_iter': 500}: \n",
      "All the 5 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\vhuta\\Desktop\\OralSmart - Data Analysis\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vhuta\\Desktop\\OralSmart - Data Analysis\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1358, in wrapper\n",
      "    estimator._validate_params()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"c:\\Users\\vhuta\\Desktop\\OralSmart - Data Analysis\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 471, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self._parameter_constraints,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.get_params(deep=False),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        caller_name=self.__class__.__name__,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\vhuta\\Desktop\\OralSmart - Data Analysis\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'solver' parameter of MLPClassifier must be a str among {'adam', 'sgd', 'lbfgs'}. Got 'adamw' instead.\n",
      "\n",
      "2025-09-06 00:02:03,698 - ERROR - Traceback (most recent call last):\n",
      "  File \"C:\\Users\\vhuta\\AppData\\Local\\Temp\\ipykernel_32588\\963565606.py\", line 36, in <module>\n",
      "    cv_scores = cross_val_score(clf, X_full, y_full, cv=cv, scoring='accuracy')\n",
      "  File \"c:\\Users\\vhuta\\Desktop\\OralSmart - Data Analysis\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 218, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\vhuta\\Desktop\\OralSmart - Data Analysis\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 677, in cross_val_score\n",
      "    cv_results = cross_validate(\n",
      "        estimator=estimator,\n",
      "    ...<9 lines>...\n",
      "        error_score=error_score,\n",
      "    )\n",
      "  File \"c:\\Users\\vhuta\\Desktop\\OralSmart - Data Analysis\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 218, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"c:\\Users\\vhuta\\Desktop\\OralSmart - Data Analysis\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 419, in cross_validate\n",
      "    _warn_or_raise_about_fit_failures(results, error_score)\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vhuta\\Desktop\\OralSmart - Data Analysis\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 505, in _warn_or_raise_about_fit_failures\n",
      "    raise ValueError(all_fits_failed_message)\n",
      "ValueError: \n",
      "All the 5 fits failed.\n",
      "It is very likely that your model is misconfigured.\n",
      "You can try to debug the error by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "5 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\vhuta\\Desktop\\OralSmart - Data Analysis\\venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 859, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "    ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vhuta\\Desktop\\OralSmart - Data Analysis\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 1358, in wrapper\n",
      "    estimator._validate_params()\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~^^\n",
      "  File \"c:\\Users\\vhuta\\Desktop\\OralSmart - Data Analysis\\venv\\Lib\\site-packages\\sklearn\\base.py\", line 471, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^\n",
      "        self._parameter_constraints,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        self.get_params(deep=False),\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        caller_name=self.__class__.__name__,\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"c:\\Users\\vhuta\\Desktop\\OralSmart - Data Analysis\\venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 98, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "    ...<2 lines>...\n",
      "    )\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'solver' parameter of MLPClassifier must be a str among {'adam', 'sgd', 'lbfgs'}. Got 'adamw' instead.\n",
      "\n",
      "\n",
      "Scikit-learn experiments: 100%|██████████| 1/1 [00:01<00:00,  1.05s/it]\n",
      "2025-09-06 00:02:03,699 - INFO - Completed 0 successful scikit-learn experiments\n"
     ]
    }
   ],
   "source": [
    "# Scikit-learn MLP experiments with Cross-Validation and Comprehensive Metrics\n",
    "sklearn_results = []\n",
    "combos = list(product(*sklearn_param_grid.values()))\n",
    "keys = list(sklearn_param_grid.keys())\n",
    "\n",
    "logger.info(f\"Starting {len(combos)} scikit-learn experiments\")\n",
    "if use_cross_validation:\n",
    "    logger.info(f\"Using {n_folds}-fold cross-validation (this will increase training time by ~{n_folds}x)\")\n",
    "\n",
    "# Combine train and test data for cross-validation\n",
    "X_full = np.vstack([X_train_s, X_test_s])\n",
    "y_full = np.concatenate([y_train, y_test])\n",
    "\n",
    "# Set up cross-validation\n",
    "if use_cross_validation:\n",
    "    cv = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    cv_splits = list(cv.split(X_full, y_full))\n",
    "\n",
    "best_sklearn_model = None\n",
    "best_sklearn_score = 0.0\n",
    "\n",
    "for i, combo in enumerate(tqdm(combos, desc=\"Scikit-learn experiments\")):\n",
    "    try:\n",
    "        params = dict(zip(keys, combo))\n",
    "        logger.debug(f\"Testing sklearn params: {params}\")\n",
    "        \n",
    "        # Log system resources\n",
    "        memory_usage = psutil.virtual_memory().percent\n",
    "        cpu_usage = psutil.cpu_percent(interval=1)\n",
    "        \n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        if use_cross_validation:\n",
    "            # Cross-validation approach\n",
    "            clf = MLPClassifier(**params, random_state=42)\n",
    "            cv_scores = cross_val_score(clf, X_full, y_full, cv=cv, scoring='accuracy')\n",
    "            \n",
    "            # Train on full dataset to get other metrics\n",
    "            clf.fit(X_full, y_full)\n",
    "            \n",
    "            # Calculate metrics on original train/test split for comparison\n",
    "            train_pred = clf.predict(X_train_s)\n",
    "            test_pred = clf.predict(X_test_s)\n",
    "            train_acc = accuracy_score(y_train, train_pred)\n",
    "            test_acc = accuracy_score(y_test, test_pred)\n",
    "            \n",
    "            cv_mean = cv_scores.mean()\n",
    "            cv_std = cv_scores.std()\n",
    "            \n",
    "        else:\n",
    "            # Original single train/test split approach\n",
    "            clf = MLPClassifier(**params, random_state=42)\n",
    "            clf.fit(X_train_s, y_train)\n",
    "            \n",
    "            train_pred = clf.predict(X_train_s)\n",
    "            test_pred = clf.predict(X_test_s)\n",
    "            train_acc = accuracy_score(y_train, train_pred)\n",
    "            test_acc = accuracy_score(y_test, test_pred)\n",
    "            \n",
    "            cv_mean = None\n",
    "            cv_std = None\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        # Calculate comprehensive metrics for test set\n",
    "        model_name = f\"sklearn_mlp_{i+1}\"\n",
    "        test_metrics = calculate_comprehensive_metrics(\n",
    "            y_test, test_pred, model_name, i+1, 'sklearn'\n",
    "        )\n",
    "        \n",
    "        # Calculate comprehensive metrics for training set\n",
    "        train_metrics = calculate_comprehensive_metrics(\n",
    "            y_train, train_pred, f\"{model_name}_train\", f\"{i+1}_train\", 'sklearn'\n",
    "        )\n",
    "        \n",
    "        # Save model\n",
    "        import joblib\n",
    "        model_path = os.path.join(OUTDIR, 'sklearn_models', f'sklearn_model_{i+1}.joblib')\n",
    "        joblib.dump(clf, model_path)\n",
    "        \n",
    "        # Track best model\n",
    "        score_for_comparison = cv_mean if use_cross_validation else test_acc\n",
    "        if score_for_comparison > best_sklearn_score:\n",
    "            best_sklearn_score = score_for_comparison\n",
    "            best_sklearn_model = {\n",
    "                'model': clf,\n",
    "                'experiment_id': i+1,\n",
    "                'params': params,\n",
    "                'test_metrics': test_metrics,\n",
    "                'train_metrics': train_metrics,\n",
    "                'model_path': model_path\n",
    "            }\n",
    "        \n",
    "        result = {\n",
    "            'experiment_id': i + 1,\n",
    "            'params': params,\n",
    "            'train_acc': train_acc,\n",
    "            'test_acc': test_acc,\n",
    "            'train_precision_macro': train_metrics['precision_macro'],\n",
    "            'train_recall_macro': train_metrics['recall_macro'],\n",
    "            'train_f1_macro': train_metrics['f1_macro'],\n",
    "            'test_precision_macro': test_metrics['precision_macro'],\n",
    "            'test_recall_macro': test_metrics['recall_macro'],\n",
    "            'test_f1_macro': test_metrics['f1_macro'],\n",
    "            'test_precision_weighted': test_metrics['precision_weighted'],\n",
    "            'test_recall_weighted': test_metrics['recall_weighted'],\n",
    "            'test_f1_weighted': test_metrics['f1_weighted'],\n",
    "            'cv_mean_acc': cv_mean,\n",
    "            'cv_std_acc': cv_std,\n",
    "            'n_iter': clf.n_iter_,\n",
    "            'loss_curve': clf.loss_curve_ if hasattr(clf, 'loss_curve_') else None,\n",
    "            'duration_seconds': duration,\n",
    "            'memory_usage_percent': memory_usage,\n",
    "            'cpu_usage_percent': cpu_usage,\n",
    "            'timestamp': start_time.isoformat(),\n",
    "            'model_path': model_path\n",
    "        }\n",
    "        \n",
    "        sklearn_results.append(result)\n",
    "        \n",
    "        if use_cross_validation:\n",
    "            logger.info(f\"Sklearn experiment {i+1}/{len(combos)} completed. CV mean: {cv_mean:.4f}±{cv_std:.4f}, Test F1: {test_metrics['f1_macro']:.4f}\")\n",
    "        else:\n",
    "            logger.info(f\"Sklearn experiment {i+1}/{len(combos)} completed. Train acc: {train_acc:.4f}, Test F1: {test_metrics['f1_macro']:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Sklearn experiment {i+1} failed with params {params}: {str(e)}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        continue\n",
    "\n",
    "logger.info(f\"Completed {len(sklearn_results)} successful scikit-learn experiments\")\n",
    "\n",
    "# Save best sklearn model with special designation\n",
    "if best_sklearn_model:\n",
    "    best_model_dir = os.path.join(OUTDIR, 'best_models')\n",
    "    os.makedirs(best_model_dir, exist_ok=True)\n",
    "    \n",
    "    # Save best model\n",
    "    best_model_path = os.path.join(best_model_dir, 'best_sklearn_model.joblib')\n",
    "    joblib.dump(best_sklearn_model['model'], best_model_path)\n",
    "    \n",
    "    # Save best model metrics with special naming\n",
    "    best_test_metrics = calculate_comprehensive_metrics(\n",
    "        y_test, best_sklearn_model['model'].predict(X_test_s), \n",
    "        'BEST_sklearn_model', 'BEST', 'sklearn_best'\n",
    "    )\n",
    "    \n",
    "    # Save best model info\n",
    "    best_info = {\n",
    "        'experiment_id': best_sklearn_model['experiment_id'],\n",
    "        'params': best_sklearn_model['params'],\n",
    "        'metrics': best_test_metrics,\n",
    "        'model_path': best_model_path,\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(best_model_dir, 'best_sklearn_model_info.json'), 'w') as f:\n",
    "        json.dump(best_info, f, indent=2)\n",
    "    \n",
    "    logger.info(f\"Best sklearn model saved (Experiment {best_sklearn_model['experiment_id']}) with score: {best_sklearn_score:.4f}\")\n",
    "\n",
    "# Save sklearn results\n",
    "if sklearn_results:\n",
    "    sklearn_df = pd.DataFrame([\n",
    "        {\n",
    "            'experiment_id': r['experiment_id'],\n",
    "            'hidden_layer_sizes': str(r['params']['hidden_layer_sizes']),\n",
    "            'solver': r['params']['solver'],\n",
    "            'learning_rate_init': r['params']['learning_rate_init'],\n",
    "            'max_iter': r['params']['max_iter'],\n",
    "            'train_acc': r['train_acc'],\n",
    "            'test_acc': r['test_acc'],\n",
    "            'train_f1_macro': r['train_f1_macro'],\n",
    "            'test_f1_macro': r['test_f1_macro'],\n",
    "            'test_precision_macro': r['test_precision_macro'],\n",
    "            'test_recall_macro': r['test_recall_macro'],\n",
    "            'test_f1_weighted': r['test_f1_weighted'],\n",
    "            'cv_mean_acc': r['cv_mean_acc'],\n",
    "            'cv_std_acc': r['cv_std_acc'],\n",
    "            'n_iter': r['n_iter'],\n",
    "            'duration_seconds': r['duration_seconds'],\n",
    "            'memory_usage_percent': r['memory_usage_percent'],\n",
    "            'cpu_usage_percent': r['cpu_usage_percent'],\n",
    "            'timestamp': r['timestamp'],\n",
    "            'model_path': r['model_path']\n",
    "        } for r in sklearn_results\n",
    "    ])\n",
    "    sklearn_df.to_csv(os.path.join(OUTDIR, 'sklearn_results_summary.csv'), index=False)\n",
    "    logger.info(\"Scikit-learn results saved to sklearn_results_summary.csv\")\n",
    "    \n",
    "    # Display best results\n",
    "    if use_cross_validation:\n",
    "        best_sklearn = sklearn_df.loc[sklearn_df['cv_mean_acc'].idxmax()]\n",
    "        print(f\"\\nBest scikit-learn result (by CV score):\")\n",
    "        print(f\"CV accuracy: {best_sklearn['cv_mean_acc']:.4f}±{best_sklearn['cv_std_acc']:.4f}\")\n",
    "        print(f\"Test accuracy: {best_sklearn['test_acc']:.4f}\")\n",
    "        print(f\"Test F1-macro: {best_sklearn['test_f1_macro']:.4f}\")\n",
    "        print(f\"Test Precision-macro: {best_sklearn['test_precision_macro']:.4f}\")\n",
    "        print(f\"Test Recall-macro: {best_sklearn['test_recall_macro']:.4f}\")\n",
    "        print(f\"Parameters: hidden_sizes={best_sklearn['hidden_layer_sizes']}, solver={best_sklearn['solver']}, lr={best_sklearn['learning_rate_init']}\")\n",
    "    else:\n",
    "        best_sklearn = sklearn_df.loc[sklearn_df['test_acc'].idxmax()]\n",
    "        print(f\"\\nBest scikit-learn result:\")\n",
    "        print(f\"Test accuracy: {best_sklearn['test_acc']:.4f}\")\n",
    "        print(f\"Test F1-macro: {best_sklearn['test_f1_macro']:.4f}\")\n",
    "        print(f\"Test Precision-macro: {best_sklearn['test_precision_macro']:.4f}\")\n",
    "        print(f\"Test Recall-macro: {best_sklearn['test_recall_macro']:.4f}\")\n",
    "        print(f\"Parameters: hidden_sizes={best_sklearn['hidden_layer_sizes']}, solver={best_sklearn['solver']}, lr={best_sklearn['learning_rate_init']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e90270f4-a097-4f5f-b1a9-740f7be89339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ PyTorch installed: 2.8.0+cu128\n",
      "✅ CUDA available: True\n",
      "✅ GPU device: NVIDIA GeForce RTX 4050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "# Add this cell to check PyTorch installation\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"✅ PyTorch installed: {torch.__version__}\")\n",
    "    print(f\"✅ CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"✅ GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        print(\"⚠️  CUDA not available - will use CPU\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ PyTorch not installed: {e}\")\n",
    "    print(\"Install with: pip install torch torchvision torchaudio\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ PyTorch error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "pytorch_setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 00:02:05,088 - INFO - MLflow experiment tracking enabled\n",
      "2025-09-06 00:02:05,089 - INFO - PyTorch device: cuda\n",
      "2025-09-06 00:02:05,089 - INFO - CUDA device: NVIDIA GeForce RTX 4050 Laptop GPU\n",
      "2025-09-06 00:02:05,090 - INFO - CUDA memory: 6.0 GB\n",
      "2025-09-06 00:02:05,093 - INFO - PyTorch setup completed successfully\n",
      "2025-09-06 00:02:05,089 - INFO - PyTorch device: cuda\n",
      "2025-09-06 00:02:05,089 - INFO - CUDA device: NVIDIA GeForce RTX 4050 Laptop GPU\n",
      "2025-09-06 00:02:05,090 - INFO - CUDA memory: 6.0 GB\n",
      "2025-09-06 00:02:05,093 - INFO - PyTorch setup completed successfully\n"
     ]
    }
   ],
   "source": [
    "# PyTorch experiments setup\n",
    "if use_pytorch:\n",
    "    try:\n",
    "        import torch\n",
    "        import torch.nn as nn\n",
    "        import torch.optim as optim\n",
    "        from torch.utils.data import TensorDataset, DataLoader\n",
    "        from torch.utils.tensorboard import SummaryWriter\n",
    "        \n",
    "        # MLflow setup (optional)\n",
    "        if use_mlflow:\n",
    "            import mlflow\n",
    "            import mlflow.pytorch\n",
    "            mlflow.set_experiment(\"MLP_Hyperparameter_Search\")\n",
    "            logger.info(\"MLflow experiment tracking enabled\")\n",
    "        \n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        logger.info(f'PyTorch device: {device}')\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            logger.info(f'CUDA device: {torch.cuda.get_device_name(0)}')\n",
    "            logger.info(f'CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB')\n",
    "        \n",
    "        def build_pytorch_model(input_size, hidden_sizes, dropout=0.0, n_classes=None):\n",
    "            layers = []\n",
    "            in_size = input_size\n",
    "            for h in hidden_sizes:\n",
    "                layers.append(nn.Linear(in_size, h))\n",
    "                layers.append(nn.ReLU())\n",
    "                if dropout and dropout > 0.0:\n",
    "                    layers.append(nn.Dropout(dropout))\n",
    "                in_size = h\n",
    "            layers.append(nn.Linear(in_size, n_classes))\n",
    "            return nn.Sequential(*layers)\n",
    "        \n",
    "        # Prepare PyTorch tensors\n",
    "        X_train_t = torch.FloatTensor(X_train_s)\n",
    "        y_train_t = torch.LongTensor(y_train.values if hasattr(y_train, 'values') else y_train)\n",
    "        X_test_t = torch.FloatTensor(X_test_s)\n",
    "        y_test_t = torch.LongTensor(y_test.values if hasattr(y_test, 'values') else y_test)\n",
    "        \n",
    "        logger.info(\"PyTorch setup completed successfully\")\n",
    "        \n",
    "    except ImportError as e:\n",
    "        logger.error(f\"PyTorch import failed: {str(e)}\")\n",
    "        use_pytorch = False\n",
    "    except Exception as e:\n",
    "        logger.error(f\"PyTorch setup failed: {str(e)}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        use_pytorch = False\n",
    "else:\n",
    "    logger.info(\"PyTorch experiments disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1743f9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 00:02:05,104 - INFO - Early stopping configured with patience=10, min_delta=0.0001\n"
     ]
    }
   ],
   "source": [
    "# Early Stopping Class for PyTorch Training\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, min_delta=0.0001, restore_best_weights=True, verbose=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                          Default: 7\n",
    "            min_delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                              Default: 0.0001\n",
    "            restore_best_weights (bool): Whether to restore model weights from the epoch with the best value.\n",
    "                                       Default: True\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                          Default: True\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = float('inf')\n",
    "        self.best_model_state = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                logger.debug(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            logger.debug(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model checkpoint...')\n",
    "        self.val_loss_min = val_loss\n",
    "        if self.restore_best_weights:\n",
    "            self.best_model_state = model.state_dict().copy()\n",
    "\n",
    "    def restore_best_model(self, model):\n",
    "        \"\"\"Restore the best model weights if available\"\"\"\n",
    "        if self.restore_best_weights and self.best_model_state is not None:\n",
    "            model.load_state_dict(self.best_model_state)\n",
    "            if self.verbose:\n",
    "                logger.info(f'Restored model to best validation loss: {self.val_loss_min:.6f}')\n",
    "\n",
    "# Early stopping configuration\n",
    "EARLY_STOPPING_PATIENCE = 10  # Stop if no improvement for 10 epochs\n",
    "EARLY_STOPPING_MIN_DELTA = 0.0001  # Minimum improvement threshold\n",
    "\n",
    "logger.info(f\"Early stopping configured with patience={EARLY_STOPPING_PATIENCE}, min_delta={EARLY_STOPPING_MIN_DELTA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91646e8",
   "metadata": {},
   "source": [
    "## Early Stopping Implementation ✅\n",
    "\n",
    "**Early stopping has been implemented** with the following features:\n",
    "\n",
    "### **Configuration:**\n",
    "- **Patience**: 10 epochs (stops training if no improvement for 10 consecutive epochs)\n",
    "- **Minimum Delta**: 0.0001 (minimum improvement threshold to be considered significant)\n",
    "- **Restore Best Weights**: Automatically restores model to the best validation loss checkpoint\n",
    "\n",
    "### **Benefits:**\n",
    "1. **Prevents Overfitting**: Stops training when validation loss stops improving\n",
    "2. **Saves Training Time**: Automatically stops when further training won't help\n",
    "3. **Optimal Performance**: Returns the model with the best validation performance\n",
    "4. **Literature Recommended**: Early stopping is a standard best practice\n",
    "\n",
    "### **Training Process:**\n",
    "- **Cross-validation**: Uses reduced patience (5 epochs) for faster fold training\n",
    "- **Final model**: Uses full patience (10 epochs) for optimal performance\n",
    "- **Validation frequency**: Every epoch (required for effective early stopping)\n",
    "\n",
    "### **Result Tracking:**\n",
    "The results now include early stopping metrics:\n",
    "- `early_stopped`: Whether early stopping was triggered\n",
    "- `epochs_trained`: Actual epochs trained (may be less than specified)\n",
    "- `best_val_loss`: Best validation loss achieved\n",
    "- `early_stop_epoch`: Epoch where early stopping occurred\n",
    "\n",
    "This implementation follows **literature best practices** and is especially beneficial for synthetic data where clear patterns allow for early convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "pytorch_experiments",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 00:02:05,157 - INFO - Starting 4 PyTorch experiments\n",
      "2025-09-06 00:02:05,158 - INFO - Using 5-fold cross-validation (this will increase training time by ~5x)\n",
      "PyTorch experiments:   0%|          | 0/4 [00:00<?, ?it/s]2025-09-06 00:02:05,158 - INFO - Using 5-fold cross-validation (this will increase training time by ~5x)\n",
      "PyTorch experiments:   0%|          | 0/4 [00:00<?, ?it/s]2025-09-06 00:02:05,161 - INFO - \n",
      "PyTorch experiment 1/4: {'hidden_sizes': [64], 'learning_rate': 0.001, 'dropout': 0.2, 'batch_size': 64, 'epochs': 100}\n",
      "2025-09-06 00:02:05,161 - INFO - \n",
      "PyTorch experiment 1/4: {'hidden_sizes': [64], 'learning_rate': 0.001, 'dropout': 0.2, 'batch_size': 64, 'epochs': 100}\n",
      "2025-09-06 00:04:11,243 - INFO - Early stopping triggered at epoch 52/100 (patience=10)\n",
      "2025-09-06 00:04:11,244 - INFO - Best validation loss: 0.020536\n",
      "2025-09-06 00:04:11,245 - INFO - Restored model to best validation loss: 0.020536\n",
      "2025-09-06 00:04:11,245 - INFO - Model weights restored to best validation loss checkpoint\n",
      "2025-09-06 00:04:11,243 - INFO - Early stopping triggered at epoch 52/100 (patience=10)\n",
      "2025-09-06 00:04:11,244 - INFO - Best validation loss: 0.020536\n",
      "2025-09-06 00:04:11,245 - INFO - Restored model to best validation loss: 0.020536\n",
      "2025-09-06 00:04:11,245 - INFO - Model weights restored to best validation loss checkpoint\n",
      "2025-09-06 00:04:11,485 - INFO - Comprehensive metrics saved for pytorch_mlp_1 (ID: 1)\n",
      "2025-09-06 00:04:11,485 - INFO - Comprehensive metrics saved for pytorch_mlp_1 (ID: 1)\n",
      "2025-09-06 00:04:11,698 - INFO - Comprehensive metrics saved for pytorch_mlp_1_train (ID: 1_train)\n",
      "2025-09-06 00:04:11,698 - INFO - Comprehensive metrics saved for pytorch_mlp_1_train (ID: 1_train)\n",
      "2025-09-06 00:04:12,307 - INFO - PyTorch experiment 1/4 completed. CV val: 0.9922±0.0018, Test F1: 0.9930\n",
      "PyTorch experiments:  25%|██▌       | 1/4 [02:07<06:21, 127.15s/it]2025-09-06 00:04:12,314 - INFO - \n",
      "PyTorch experiment 2/4: {'hidden_sizes': [64], 'learning_rate': 0.001, 'dropout': 0.2, 'batch_size': 64, 'epochs': 150}\n",
      "2025-09-06 00:04:12,307 - INFO - PyTorch experiment 1/4 completed. CV val: 0.9922±0.0018, Test F1: 0.9930\n",
      "PyTorch experiments:  25%|██▌       | 1/4 [02:07<06:21, 127.15s/it]2025-09-06 00:04:12,314 - INFO - \n",
      "PyTorch experiment 2/4: {'hidden_sizes': [64], 'learning_rate': 0.001, 'dropout': 0.2, 'batch_size': 64, 'epochs': 150}\n",
      "2025-09-06 00:06:24,549 - INFO - Early stopping triggered at epoch 75/150 (patience=10)\n",
      "2025-09-06 00:06:24,550 - INFO - Best validation loss: 0.016041\n",
      "2025-09-06 00:06:24,551 - INFO - Restored model to best validation loss: 0.016041\n",
      "2025-09-06 00:06:24,551 - INFO - Model weights restored to best validation loss checkpoint\n",
      "2025-09-06 00:06:24,549 - INFO - Early stopping triggered at epoch 75/150 (patience=10)\n",
      "2025-09-06 00:06:24,550 - INFO - Best validation loss: 0.016041\n",
      "2025-09-06 00:06:24,551 - INFO - Restored model to best validation loss: 0.016041\n",
      "2025-09-06 00:06:24,551 - INFO - Model weights restored to best validation loss checkpoint\n",
      "2025-09-06 00:06:24,767 - INFO - Comprehensive metrics saved for pytorch_mlp_2 (ID: 2)\n",
      "2025-09-06 00:06:24,767 - INFO - Comprehensive metrics saved for pytorch_mlp_2 (ID: 2)\n",
      "2025-09-06 00:06:24,986 - INFO - Comprehensive metrics saved for pytorch_mlp_2_train (ID: 2_train)\n",
      "2025-09-06 00:06:24,986 - INFO - Comprehensive metrics saved for pytorch_mlp_2_train (ID: 2_train)\n",
      "2025-09-06 00:06:25,600 - INFO - PyTorch experiment 2/4 completed. CV val: 0.9930±0.0016, Test F1: 0.9930\n",
      "PyTorch experiments:  50%|█████     | 2/4 [04:20<04:21, 130.76s/it]2025-09-06 00:06:25,608 - INFO - \n",
      "PyTorch experiment 3/4: {'hidden_sizes': [64], 'learning_rate': 0.001, 'dropout': 0.2, 'batch_size': 128, 'epochs': 100}\n",
      "2025-09-06 00:06:25,600 - INFO - PyTorch experiment 2/4 completed. CV val: 0.9930±0.0016, Test F1: 0.9930\n",
      "PyTorch experiments:  50%|█████     | 2/4 [04:20<04:21, 130.76s/it]2025-09-06 00:06:25,608 - INFO - \n",
      "PyTorch experiment 3/4: {'hidden_sizes': [64], 'learning_rate': 0.001, 'dropout': 0.2, 'batch_size': 128, 'epochs': 100}\n",
      "2025-09-06 00:08:17,709 - INFO - Early stopping triggered at epoch 98/100 (patience=10)\n",
      "2025-09-06 00:08:17,709 - INFO - Best validation loss: 0.018760\n",
      "2025-09-06 00:08:17,710 - INFO - Restored model to best validation loss: 0.018760\n",
      "2025-09-06 00:08:17,711 - INFO - Model weights restored to best validation loss checkpoint\n",
      "2025-09-06 00:08:17,709 - INFO - Early stopping triggered at epoch 98/100 (patience=10)\n",
      "2025-09-06 00:08:17,709 - INFO - Best validation loss: 0.018760\n",
      "2025-09-06 00:08:17,710 - INFO - Restored model to best validation loss: 0.018760\n",
      "2025-09-06 00:08:17,711 - INFO - Model weights restored to best validation loss checkpoint\n",
      "2025-09-06 00:08:17,923 - INFO - Comprehensive metrics saved for pytorch_mlp_3 (ID: 3)\n",
      "2025-09-06 00:08:17,923 - INFO - Comprehensive metrics saved for pytorch_mlp_3 (ID: 3)\n",
      "2025-09-06 00:08:18,139 - INFO - Comprehensive metrics saved for pytorch_mlp_3_train (ID: 3_train)\n",
      "2025-09-06 00:08:18,139 - INFO - Comprehensive metrics saved for pytorch_mlp_3_train (ID: 3_train)\n",
      "2025-09-06 00:08:18,737 - INFO - PyTorch experiment 3/4 completed. CV val: 0.9921±0.0015, Test F1: 0.9914\n",
      "PyTorch experiments:  75%|███████▌  | 3/4 [06:13<02:02, 122.72s/it]2025-09-06 00:08:18,744 - INFO - \n",
      "PyTorch experiment 4/4: {'hidden_sizes': [64], 'learning_rate': 0.001, 'dropout': 0.2, 'batch_size': 128, 'epochs': 150}\n",
      "2025-09-06 00:08:18,737 - INFO - PyTorch experiment 3/4 completed. CV val: 0.9921±0.0015, Test F1: 0.9914\n",
      "PyTorch experiments:  75%|███████▌  | 3/4 [06:13<02:02, 122.72s/it]2025-09-06 00:08:18,744 - INFO - \n",
      "PyTorch experiment 4/4: {'hidden_sizes': [64], 'learning_rate': 0.001, 'dropout': 0.2, 'batch_size': 128, 'epochs': 150}\n",
      "2025-09-06 00:10:09,781 - INFO - Early stopping triggered at epoch 78/150 (patience=10)\n",
      "2025-09-06 00:10:09,782 - INFO - Best validation loss: 0.018331\n",
      "2025-09-06 00:10:09,782 - INFO - Restored model to best validation loss: 0.018331\n",
      "2025-09-06 00:10:09,783 - INFO - Model weights restored to best validation loss checkpoint\n",
      "2025-09-06 00:10:09,781 - INFO - Early stopping triggered at epoch 78/150 (patience=10)\n",
      "2025-09-06 00:10:09,782 - INFO - Best validation loss: 0.018331\n",
      "2025-09-06 00:10:09,782 - INFO - Restored model to best validation loss: 0.018331\n",
      "2025-09-06 00:10:09,783 - INFO - Model weights restored to best validation loss checkpoint\n",
      "2025-09-06 00:10:10,013 - INFO - Comprehensive metrics saved for pytorch_mlp_4 (ID: 4)\n",
      "2025-09-06 00:10:10,013 - INFO - Comprehensive metrics saved for pytorch_mlp_4 (ID: 4)\n",
      "2025-09-06 00:10:10,227 - INFO - Comprehensive metrics saved for pytorch_mlp_4_train (ID: 4_train)\n",
      "2025-09-06 00:10:10,227 - INFO - Comprehensive metrics saved for pytorch_mlp_4_train (ID: 4_train)\n",
      "2025-09-06 00:10:10,849 - INFO - PyTorch experiment 4/4 completed. CV val: 0.9925±0.0017, Test F1: 0.9936\n",
      "PyTorch experiments: 100%|██████████| 4/4 [08:05<00:00, 121.42s/it]\n",
      "2025-09-06 00:10:10,856 - INFO - Completed 4 successful PyTorch experiments\n",
      "2025-09-06 00:10:10,849 - INFO - PyTorch experiment 4/4 completed. CV val: 0.9925±0.0017, Test F1: 0.9936\n",
      "PyTorch experiments: 100%|██████████| 4/4 [08:05<00:00, 121.42s/it]\n",
      "2025-09-06 00:10:10,856 - INFO - Completed 4 successful PyTorch experiments\n",
      "2025-09-06 00:10:11,066 - INFO - Comprehensive metrics saved for BEST_pytorch_model (ID: BEST)\n",
      "2025-09-06 00:10:11,067 - INFO - Best PyTorch model saved (Experiment 2) with score: 0.9930\n",
      "2025-09-06 00:10:11,072 - INFO - PyTorch results saved to pytorch_results_summary.csv\n",
      "2025-09-06 00:10:11,066 - INFO - Comprehensive metrics saved for BEST_pytorch_model (ID: BEST)\n",
      "2025-09-06 00:10:11,067 - INFO - Best PyTorch model saved (Experiment 2) with score: 0.9930\n",
      "2025-09-06 00:10:11,072 - INFO - PyTorch results saved to pytorch_results_summary.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best PyTorch result (by CV score):\n",
      "CV val accuracy: 0.9930±0.0016\n",
      "Test accuracy: 0.9930\n",
      "Test F1-macro: 0.9930\n",
      "Test Precision-macro: 0.9934\n",
      "Test Recall-macro: 0.9927\n",
      "Parameters: hidden_sizes=[64], lr=0.001, dropout=0.2\n"
     ]
    }
   ],
   "source": [
    "# Enhanced PyTorch experiments with Cross-Validation and Comprehensive Metrics\n",
    "if use_pytorch:\n",
    "    pytorch_results = []\n",
    "    combos = list(product(*pytorch_param_grid.values()))\n",
    "    keys = list(pytorch_param_grid.keys())\n",
    "    \n",
    "    logger.info(f\"Starting {len(combos)} PyTorch experiments\")\n",
    "    if use_cross_validation:\n",
    "        logger.info(f\"Using {n_folds}-fold cross-validation (this will increase training time by ~{n_folds}x)\")\n",
    "    \n",
    "    run_counter = 0\n",
    "    best_test_acc = 0.0\n",
    "    best_pytorch_model = None\n",
    "    \n",
    "    # Prepare full dataset for cross-validation\n",
    "    X_full_t = torch.FloatTensor(X_full)\n",
    "    y_full_t = torch.LongTensor(y_full)\n",
    "    \n",
    "    for combo in tqdm(combos, desc=\"PyTorch experiments\"):\n",
    "        run_counter += 1\n",
    "        \n",
    "        try:\n",
    "            params = dict(zip(keys, combo))\n",
    "            logger.info(f\"\\nPyTorch experiment {run_counter}/{len(combos)}: {params}\")\n",
    "            \n",
    "            # MLflow run start\n",
    "            if use_mlflow:\n",
    "                mlflow.start_run()\n",
    "                mlflow.log_params(params)\n",
    "            \n",
    "            # Cross-validation scores storage\n",
    "            cv_train_accs = []\n",
    "            cv_val_accs = []\n",
    "            cv_final_losses = []\n",
    "            \n",
    "            if use_cross_validation:\n",
    "                # Cross-validation loop\n",
    "                for fold, (train_idx, val_idx) in enumerate(cv_splits):\n",
    "                    logger.debug(f\"Training fold {fold + 1}/{n_folds}\")\n",
    "                    \n",
    "                    # Split data for this fold\n",
    "                    X_fold_train = X_full_t[train_idx]\n",
    "                    y_fold_train = y_full_t[train_idx]\n",
    "                    X_fold_val = X_full_t[val_idx]\n",
    "                    y_fold_val = y_full_t[val_idx]\n",
    "                    \n",
    "                    # Model setup for this fold\n",
    "                    input_size = X_full.shape[1]\n",
    "                    n_classes = len(np.unique(y_full))\n",
    "                    model = build_pytorch_model(\n",
    "                        input_size, \n",
    "                        params['hidden_sizes'], \n",
    "                        dropout=params['dropout'], \n",
    "                        n_classes=n_classes\n",
    "                    ).to(device)\n",
    "                    \n",
    "                    criterion = nn.CrossEntropyLoss()\n",
    "                    optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "                    \n",
    "                    # Data loader for this fold\n",
    "                    fold_train_ds = TensorDataset(X_fold_train, y_fold_train)\n",
    "                    fold_train_loader = DataLoader(fold_train_ds, batch_size=params['batch_size'], shuffle=True)\n",
    "                    \n",
    "                    # Training loop for this fold with early stopping\n",
    "                    fold_early_stopping = EarlyStopping(\n",
    "                        patience=EARLY_STOPPING_PATIENCE//2,  # Reduced patience for CV folds\n",
    "                        min_delta=EARLY_STOPPING_MIN_DELTA,\n",
    "                        verbose=False  # Less verbose for CV folds\n",
    "                    )\n",
    "                    \n",
    "                    for epoch in range(params['epochs']):\n",
    "                        model.train()\n",
    "                        epoch_losses = []\n",
    "                        \n",
    "                        for batch_idx, (xb, yb) in enumerate(fold_train_loader):\n",
    "                            xb = xb.to(device)\n",
    "                            yb = yb.to(device)\n",
    "                            \n",
    "                            optimizer.zero_grad()\n",
    "                            out = model(xb)\n",
    "                            loss = criterion(out, yb)\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "                            \n",
    "                            epoch_losses.append(loss.item())\n",
    "                        \n",
    "                        # Validation on fold validation set every epoch for early stopping\n",
    "                        model.eval()\n",
    "                        with torch.no_grad():\n",
    "                            fold_val_out = model(X_fold_val.to(device))\n",
    "                            fold_val_loss = criterion(fold_val_out, y_fold_val.to(device))\n",
    "                        \n",
    "                        # Early stopping check\n",
    "                        fold_early_stopping(fold_val_loss.item(), model)\n",
    "                        \n",
    "                        if fold_early_stopping.early_stop:\n",
    "                            logger.debug(f\"Fold {fold+1} early stopping at epoch {epoch+1}\")\n",
    "                            break\n",
    "                    \n",
    "                    # Restore best model for this fold\n",
    "                    if fold_early_stopping.restore_best_weights:\n",
    "                        fold_early_stopping.restore_best_model(model)\n",
    "                    \n",
    "                    # Evaluate this fold\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        # Training accuracy for this fold\n",
    "                        fold_train_out = model(X_fold_train.to(device))\n",
    "                        fold_train_pred = fold_train_out.argmax(dim=1).cpu().numpy()\n",
    "                        fold_train_acc = accuracy_score(y_fold_train.cpu().numpy(), fold_train_pred)\n",
    "                        \n",
    "                        # Validation accuracy for this fold\n",
    "                        fold_val_out = model(X_fold_val.to(device))\n",
    "                        fold_val_pred = fold_val_out.argmax(dim=1).cpu().numpy()\n",
    "                        fold_val_acc = accuracy_score(y_fold_val.cpu().numpy(), fold_val_pred)\n",
    "                        fold_val_loss = criterion(fold_val_out, y_fold_val.to(device)).item()\n",
    "                    \n",
    "                    cv_train_accs.append(fold_train_acc)\n",
    "                    cv_val_accs.append(fold_val_acc)\n",
    "                    cv_final_losses.append(fold_val_loss)\n",
    "                    \n",
    "                    logger.debug(f\"Fold {fold + 1}: train_acc={fold_train_acc:.4f}, val_acc={fold_val_acc:.4f}\")\n",
    "                \n",
    "                # Calculate cross-validation statistics\n",
    "                cv_train_mean = np.mean(cv_train_accs)\n",
    "                cv_train_std = np.std(cv_train_accs)\n",
    "                cv_val_mean = np.mean(cv_val_accs)\n",
    "                cv_val_std = np.std(cv_val_accs)\n",
    "                cv_loss_mean = np.mean(cv_final_losses)\n",
    "            \n",
    "            else:\n",
    "                # No cross-validation\n",
    "                cv_train_mean = cv_train_std = cv_val_mean = cv_val_std = cv_loss_mean = None\n",
    "            \n",
    "            # Train final model on original train/test split\n",
    "            model = build_pytorch_model(\n",
    "                X_train_s.shape[1], \n",
    "                params['hidden_sizes'], \n",
    "                dropout=params['dropout'], \n",
    "                n_classes=len(np.unique(y_train))\n",
    "            ).to(device)\n",
    "            \n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "            \n",
    "            train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "            train_loader = DataLoader(train_ds, batch_size=params['batch_size'], shuffle=True)\n",
    "            \n",
    "            # Setup logging for final model\n",
    "            log_dir = os.path.join(OUTDIR, f'tb_logs/run_{run_counter}')\n",
    "            writer = SummaryWriter(log_dir=log_dir)\n",
    "            \n",
    "            train_losses = []\n",
    "            val_losses = []\n",
    "            val_accuracies = []\n",
    "            start_time = datetime.now()\n",
    "            \n",
    "            # Training loop for final model with early stopping\n",
    "            early_stopping = EarlyStopping(\n",
    "                patience=EARLY_STOPPING_PATIENCE,\n",
    "                min_delta=EARLY_STOPPING_MIN_DELTA,\n",
    "                verbose=True\n",
    "            )\n",
    "            \n",
    "            val_epochs = []  # Track which epochs we validated\n",
    "            \n",
    "            for epoch in range(params['epochs']):\n",
    "                model.train()\n",
    "                epoch_losses = []\n",
    "                \n",
    "                for batch_idx, (xb, yb) in enumerate(train_loader):\n",
    "                    xb = xb.to(device)\n",
    "                    yb = yb.to(device)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    out = model(xb)\n",
    "                    loss = criterion(out, yb)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    epoch_losses.append(loss.item())\n",
    "                \n",
    "                avg_train_loss = np.mean(epoch_losses)\n",
    "                train_losses.append(avg_train_loss)\n",
    "                \n",
    "                # Validation every epoch for early stopping\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    val_out = model(X_test_t.to(device))\n",
    "                    val_loss = criterion(val_out, y_test_t.to(device))\n",
    "                    val_pred = val_out.argmax(dim=1).cpu().numpy()\n",
    "                    val_acc = accuracy_score(y_test, val_pred)\n",
    "                \n",
    "                val_losses.append(val_loss.item())\n",
    "                val_accuracies.append(val_acc)\n",
    "                val_epochs.append(epoch)\n",
    "                \n",
    "                # Early stopping check\n",
    "                early_stopping(val_loss.item(), model)\n",
    "                \n",
    "                # Logging every 5 epochs or at early stop\n",
    "                if epoch % 5 == 0 or epoch == params['epochs'] - 1 or early_stopping.early_stop:\n",
    "                    # TensorBoard logging\n",
    "                    writer.add_scalar('Loss/train', avg_train_loss, epoch)\n",
    "                    writer.add_scalar('Loss/validation', val_loss.item(), epoch)\n",
    "                    writer.add_scalar('Accuracy/validation', val_acc, epoch)\n",
    "                    \n",
    "                    # MLflow logging\n",
    "                    if use_mlflow:\n",
    "                        mlflow.log_metric(\"train_loss\", avg_train_loss, step=epoch)\n",
    "                        mlflow.log_metric(\"val_loss\", val_loss.item(), step=epoch)\n",
    "                        mlflow.log_metric(\"val_accuracy\", val_acc, step=epoch)\n",
    "                    \n",
    "                    # System resource monitoring\n",
    "                    memory_usage = psutil.virtual_memory().percent\n",
    "                    if torch.cuda.is_available():\n",
    "                        gpu_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "                        writer.add_scalar('System/gpu_memory_gb', gpu_memory, epoch)\n",
    "                    writer.add_scalar('System/memory_usage_percent', memory_usage, epoch)\n",
    "                \n",
    "                # Progress reporting\n",
    "                if (epoch + 1) % 20 == 0 or epoch == 0 or early_stopping.early_stop:\n",
    "                    logger.debug(f\"Epoch {epoch+1}/{params['epochs']}: train_loss={avg_train_loss:.4f}, val_loss={val_loss.item():.4f}\")\n",
    "                \n",
    "                # Early stopping break\n",
    "                if early_stopping.early_stop:\n",
    "                    logger.info(f\"Early stopping triggered at epoch {epoch+1}/{params['epochs']} (patience={EARLY_STOPPING_PATIENCE})\")\n",
    "                    logger.info(f\"Best validation loss: {early_stopping.val_loss_min:.6f}\")\n",
    "                    break\n",
    "            \n",
    "            # Restore best model if early stopping was used\n",
    "            if early_stopping.restore_best_weights:\n",
    "                early_stopping.restore_best_model(model)\n",
    "                logger.info(\"Model weights restored to best validation loss checkpoint\")\n",
    "            \n",
    "            # Final evaluation\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                out_train = model(X_train_t.to(device))\n",
    "                pred_train = out_train.argmax(dim=1).cpu().numpy()\n",
    "                out_test = model(X_test_t.to(device))\n",
    "                pred_test = out_test.argmax(dim=1).cpu().numpy()\n",
    "            \n",
    "            train_acc = accuracy_score(y_train, pred_train)\n",
    "            test_acc = accuracy_score(y_test, pred_test)\n",
    "            \n",
    "            end_time = datetime.now()\n",
    "            duration = (end_time - start_time).total_seconds()\n",
    "            \n",
    "            # Calculate comprehensive metrics\n",
    "            model_name = f\"pytorch_mlp_{run_counter}\"\n",
    "            test_metrics = calculate_comprehensive_metrics(\n",
    "                y_test, pred_test, model_name, run_counter, 'pytorch'\n",
    "            )\n",
    "            \n",
    "            train_metrics = calculate_comprehensive_metrics(\n",
    "                y_train, pred_train, f\"{model_name}_train\", f\"{run_counter}_train\", 'pytorch'\n",
    "            )\n",
    "            \n",
    "            # Save model\n",
    "            model_path = os.path.join(OUTDIR, 'pytorch_models', f'pytorch_model_{run_counter}.pth')\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'params': params,\n",
    "                'test_acc': test_acc,\n",
    "                'test_metrics': test_metrics,\n",
    "                'cv_val_mean': cv_val_mean if use_cross_validation else None,\n",
    "                'run': run_counter\n",
    "            }, model_path)\n",
    "            \n",
    "            # Track best model\n",
    "            score_for_comparison = cv_val_mean if use_cross_validation else test_acc\n",
    "            if score_for_comparison and score_for_comparison > best_test_acc:\n",
    "                best_test_acc = score_for_comparison\n",
    "                best_pytorch_model = {\n",
    "                    'model': model,\n",
    "                    'experiment_id': run_counter,\n",
    "                    'params': params,\n",
    "                    'test_metrics': test_metrics,\n",
    "                    'train_metrics': train_metrics,\n",
    "                    'model_path': model_path\n",
    "                }\n",
    "            \n",
    "            # Log cross-validation metrics to MLflow\n",
    "            if use_mlflow and use_cross_validation:\n",
    "                if cv_train_mean is not None:\n",
    "                    mlflow.log_metric(\"cv_train_mean\", float(cv_train_mean))\n",
    "                if cv_train_std is not None:\n",
    "                    mlflow.log_metric(\"cv_train_std\", float(cv_train_std))\n",
    "                if cv_val_mean is not None:\n",
    "                    mlflow.log_metric(\"cv_val_mean\", float(cv_val_mean))\n",
    "                if cv_val_std is not None:\n",
    "                    mlflow.log_metric(\"cv_val_std\", float(cv_val_std))\n",
    "                mlflow.log_metric(\"final_train_acc\", float(train_acc))\n",
    "                mlflow.log_metric(\"final_test_acc\", float(test_acc))\n",
    "            \n",
    "            # Store results\n",
    "            result = {\n",
    "                'run': run_counter,\n",
    "                'params': params,\n",
    "                'train_acc': train_acc,\n",
    "                'test_acc': test_acc,\n",
    "                'train_precision_macro': train_metrics['precision_macro'],\n",
    "                'train_recall_macro': train_metrics['recall_macro'],\n",
    "                'train_f1_macro': train_metrics['f1_macro'],\n",
    "                'test_precision_macro': test_metrics['precision_macro'],\n",
    "                'test_recall_macro': test_metrics['recall_macro'],\n",
    "                'test_f1_macro': test_metrics['f1_macro'],\n",
    "                'test_precision_weighted': test_metrics['precision_weighted'],\n",
    "                'test_recall_weighted': test_metrics['recall_weighted'],\n",
    "                'test_f1_weighted': test_metrics['f1_weighted'],\n",
    "                'cv_train_mean': cv_train_mean,\n",
    "                'cv_train_std': cv_train_std,\n",
    "                'cv_val_mean': cv_val_mean,\n",
    "                'cv_val_std': cv_val_std,\n",
    "                'cv_loss_mean': cv_loss_mean,\n",
    "                'final_train_loss': train_losses[-1] if train_losses else None,\n",
    "                'final_val_loss': val_losses[-1] if val_losses else None,\n",
    "                'best_val_loss': early_stopping.val_loss_min,\n",
    "                'early_stopped': early_stopping.early_stop,\n",
    "                'epochs_trained': len(train_losses),  # Actual epochs trained (may be less due to early stopping)\n",
    "                'early_stop_epoch': len(train_losses) if early_stopping.early_stop else None,\n",
    "                'duration_seconds': duration,\n",
    "                'timestamp': start_time.isoformat(),\n",
    "                'model_path': model_path\n",
    "            }\n",
    "            \n",
    "            pytorch_results.append(result)\n",
    "            \n",
    "            # Confusion matrix logging\n",
    "            cm = confusion_matrix(y_test, pred_test)\n",
    "            fig, ax = plt.subplots(figsize=(8, 6))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap='Blues')\n",
    "            ax.set_title(f'Confusion Matrix - Run {run_counter}')\n",
    "            ax.set_xlabel('Predicted')\n",
    "            ax.set_ylabel('Actual')\n",
    "            writer.add_figure('Confusion_Matrix', fig, global_step=run_counter)\n",
    "            plt.close(fig)\n",
    "            \n",
    "            # Training loss curve plot with fixed dimension matching\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "            \n",
    "            # Plot training losses (one per epoch)\n",
    "            ax1.plot(range(len(train_losses)), train_losses, label='Training Loss')\n",
    "            \n",
    "            # Plot validation losses with correct epochs alignment\n",
    "            if val_losses and len(val_losses) > 0:\n",
    "                # val_epochs contains the actual epochs where validation occurred\n",
    "                # val_losses contains the corresponding validation losses\n",
    "                ax1.plot(val_epochs, val_losses, label='Validation Loss', marker='o')\n",
    "            \n",
    "            ax1.set_title(f\"Loss Curves - Run {run_counter}\")\n",
    "            ax1.set_xlabel('Epoch')\n",
    "            ax1.set_ylabel('Loss')\n",
    "            ax1.legend()\n",
    "            ax1.grid(True)\n",
    "            \n",
    "            # Plot validation accuracies\n",
    "            if val_accuracies and len(val_accuracies) > 0:\n",
    "                ax2.plot(val_epochs, val_accuracies, label='Validation Accuracy', marker='o', color='green')\n",
    "                ax2.set_title(f\"Validation Accuracy - Run {run_counter}\")\n",
    "                ax2.set_xlabel('Epoch')\n",
    "                ax2.set_ylabel('Accuracy')\n",
    "                ax2.legend()\n",
    "                ax2.grid(True)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save plot\n",
    "            fname = f\"pytorch_metrics_run_{run_counter}.png\"\n",
    "            plt.savefig(os.path.join(OUTDIR, fname), dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # Progress logging\n",
    "            if use_cross_validation:\n",
    "                logger.info(f\"PyTorch experiment {run_counter}/{len(combos)} completed. CV val: {cv_val_mean:.4f}±{cv_val_std:.4f}, Test F1: {test_metrics['f1_macro']:.4f}\")\n",
    "            else:\n",
    "                logger.info(f\"PyTorch experiment {run_counter}/{len(combos)} completed. Train acc: {train_acc:.4f}, Test F1: {test_metrics['f1_macro']:.4f}\")\n",
    "            \n",
    "            # Clean up\n",
    "            writer.close()\n",
    "            if use_mlflow:\n",
    "                mlflow.end_run()\n",
    "            \n",
    "            # Memory cleanup\n",
    "            del model\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        except Exception as e:\n",
    "            logger.error(f\"PyTorch experiment {run_counter} failed: {str(e)}\")\n",
    "            logger.error(traceback.format_exc())\n",
    "            if use_mlflow:\n",
    "                mlflow.end_run(status=\"FAILED\")\n",
    "            continue\n",
    "    \n",
    "    logger.info(f\"Completed {len(pytorch_results)} successful PyTorch experiments\")\n",
    "    \n",
    "    # Save best PyTorch model with special designation\n",
    "    if best_pytorch_model:\n",
    "        best_model_dir = os.path.join(OUTDIR, 'best_models')\n",
    "        os.makedirs(best_model_dir, exist_ok=True)\n",
    "        \n",
    "        # Save best model\n",
    "        best_model_path = os.path.join(best_model_dir, 'best_pytorch_model.pth')\n",
    "        torch.save({\n",
    "            'model_state_dict': best_pytorch_model['model'].state_dict(),\n",
    "            'params': best_pytorch_model['params'],\n",
    "            'test_metrics': best_pytorch_model['test_metrics'],\n",
    "            'run': best_pytorch_model['experiment_id']\n",
    "        }, best_model_path)\n",
    "        \n",
    "        # Save best model metrics with special naming\n",
    "        with torch.no_grad():\n",
    "            best_pred = best_pytorch_model['model'](X_test_t.to(device)).argmax(dim=1).cpu().numpy()\n",
    "        \n",
    "        best_test_metrics = calculate_comprehensive_metrics(\n",
    "            y_test, best_pred, 'BEST_pytorch_model', 'BEST', 'pytorch_best'\n",
    "        )\n",
    "        \n",
    "        # Save best model info\n",
    "        best_info = {\n",
    "            'experiment_id': best_pytorch_model['experiment_id'],\n",
    "            'params': best_pytorch_model['params'],\n",
    "            'metrics': best_test_metrics,\n",
    "            'model_path': best_model_path,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(best_model_dir, 'best_pytorch_model_info.json'), 'w') as f:\n",
    "            json.dump(best_info, f, indent=2)\n",
    "        \n",
    "        logger.info(f\"Best PyTorch model saved (Experiment {best_pytorch_model['experiment_id']}) with score: {best_test_acc:.4f}\")\n",
    "    \n",
    "    # Save PyTorch results\n",
    "    if pytorch_results:\n",
    "        pytorch_df = pd.DataFrame([\n",
    "            {\n",
    "                'run': r['run'],\n",
    "                'hidden_sizes': str(r['params']['hidden_sizes']),\n",
    "                'learning_rate': r['params']['learning_rate'],\n",
    "                'dropout': r['params']['dropout'],\n",
    "                'batch_size': r['params']['batch_size'],\n",
    "                'epochs': r['params']['epochs'],\n",
    "                'train_acc': r['train_acc'],\n",
    "                'test_acc': r['test_acc'],\n",
    "                'train_f1_macro': r['train_f1_macro'],\n",
    "                'test_f1_macro': r['test_f1_macro'],\n",
    "                'test_precision_macro': r['test_precision_macro'],\n",
    "                'test_recall_macro': r['test_recall_macro'],\n",
    "                'test_f1_weighted': r['test_f1_weighted'],\n",
    "                'cv_train_mean': r['cv_train_mean'],\n",
    "                'cv_train_std': r['cv_train_std'],\n",
    "                'cv_val_mean': r['cv_val_mean'],\n",
    "                'cv_val_std': r['cv_val_std'],\n",
    "                'cv_loss_mean': r['cv_loss_mean'],\n",
    "                'final_train_loss': r['final_train_loss'],\n",
    "                'final_val_loss': r['final_val_loss'],\n",
    "                'best_val_loss': r.get('best_val_loss', None),\n",
    "                'early_stopped': r.get('early_stopped', False),\n",
    "                'epochs_trained': r.get('epochs_trained', r['params']['epochs']),\n",
    "                'early_stop_epoch': r.get('early_stop_epoch', None),\n",
    "                'duration_seconds': r['duration_seconds'],\n",
    "                'timestamp': r['timestamp'],\n",
    "                'model_path': r['model_path']\n",
    "            } for r in pytorch_results\n",
    "        ])\n",
    "        pytorch_df.to_csv(os.path.join(OUTDIR, 'pytorch_results_summary.csv'), index=False)\n",
    "        logger.info(\"PyTorch results saved to pytorch_results_summary.csv\")\n",
    "        \n",
    "        # Display best results\n",
    "        if use_cross_validation:\n",
    "            best_pytorch = pytorch_df.loc[pytorch_df['cv_val_mean'].idxmax()]\n",
    "            print(f\"\\nBest PyTorch result (by CV score):\")\n",
    "            print(f\"CV val accuracy: {best_pytorch['cv_val_mean']:.4f}±{best_pytorch['cv_val_std']:.4f}\")\n",
    "            print(f\"Test accuracy: {best_pytorch['test_acc']:.4f}\")\n",
    "            print(f\"Test F1-macro: {best_pytorch['test_f1_macro']:.4f}\")\n",
    "            print(f\"Test Precision-macro: {best_pytorch['test_precision_macro']:.4f}\")\n",
    "            print(f\"Test Recall-macro: {best_pytorch['test_recall_macro']:.4f}\")\n",
    "            print(f\"Parameters: hidden_sizes={best_pytorch['hidden_sizes']}, lr={best_pytorch['learning_rate']}, dropout={best_pytorch['dropout']}\")\n",
    "        else:\n",
    "            best_pytorch = pytorch_df.loc[pytorch_df['test_acc'].idxmax()]\n",
    "            print(f\"\\nBest PyTorch result:\")\n",
    "            print(f\"Test accuracy: {best_pytorch['test_acc']:.4f}\")\n",
    "            print(f\"Test F1-macro: {best_pytorch['test_f1_macro']:.4f}\")\n",
    "            print(f\"Test Precision-macro: {best_pytorch['test_precision_macro']:.4f}\")\n",
    "            print(f\"Test Recall-macro: {best_pytorch['test_recall_macro']:.4f}\")\n",
    "            print(f\"Parameters: hidden_sizes={best_pytorch['hidden_sizes']}, lr={best_pytorch['learning_rate']}, dropout={best_pytorch['dropout']}\")\n",
    "\n",
    "else:\n",
    "    logger.info(\"PyTorch experiments skipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "plotting_results",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 00:10:11,098 - INFO - Starting comprehensive analysis and plotting of all metrics\n",
      "2025-09-06 00:10:12,071 - INFO - Saved comprehensive metrics plot: pytorch_comprehensive_metrics.png\n",
      "2025-09-06 00:10:12,071 - INFO - Saved comprehensive metrics plot: pytorch_comprehensive_metrics.png\n",
      "2025-09-06 00:10:12,534 - INFO - Saved CV analysis plot: pytorch_cv_analysis.png\n",
      "2025-09-06 00:10:12,537 - INFO - Comprehensive analysis complete.\n",
      "2025-09-06 00:10:12,534 - INFO - Saved CV analysis plot: pytorch_cv_analysis.png\n",
      "2025-09-06 00:10:12,537 - INFO - Comprehensive analysis complete.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PYTORCH COMPREHENSIVE RESULTS SUMMARY\n",
      "============================================================\n",
      "Total experiments: 4\n",
      "\n",
      "Best model (by CV accuracy):\n",
      "  CV accuracy: 0.9930±0.0016\n",
      "  test_acc: 0.9930\n",
      "  test_f1_macro: 0.9930\n",
      "  test_precision_macro: 0.9934\n",
      "  test_recall_macro: 0.9927\n",
      "  Parameters: [64], lr=0.001, dropout=0.2\n",
      "\n",
      "Overall Performance Statistics:\n",
      "  test_acc: 0.9927±0.0009 (best: 0.9934)\n",
      "  test_f1_macro: 0.9928±0.0009 (best: 0.9936)\n",
      "  test_precision_macro: 0.9933±0.0009 (best: 0.9942)\n",
      "  test_recall_macro: 0.9923±0.0010 (best: 0.9931)\n",
      "\n",
      "============================================================\n",
      "TRAINING TIME AND COMPUTATIONAL ANALYSIS\n",
      "============================================================\n",
      "PyTorch:\n",
      "  Average time per experiment: 31.37s\n",
      "  Total time: 125.49s (2.1 minutes)\n",
      "\n",
      "Cross-validation impact:\n",
      "  Estimated 5x time increase due to CV\n",
      "  PyTorch estimated time without CV: 6.27s per experiment\n",
      "\n",
      "============================================================\n",
      "FILES AND OUTPUTS SUMMARY\n",
      "============================================================\n",
      "Generated files and directories:\n",
      "📁 experiment_outputs/\n",
      "  ├── 📁 sklearn_models/ - Individual sklearn models (.joblib)\n",
      "  ├── 📁 pytorch_models/ - Individual PyTorch models (.pth)\n",
      "  ├── 📁 best_models/ - Best models from each framework\n",
      "  ├── 📁 metrics/ - Comprehensive metrics for each model (.json)\n",
      "  ├── 📁 confusion_matrices/ - Confusion matrix plots (.png)\n",
      "  ├── 📁 tb_logs/ - TensorBoard logs (PyTorch only)\n",
      "  ├── 📄 sklearn_results_summary.csv - Sklearn experiment results\n",
      "  ├── 📄 pytorch_results_summary.csv - PyTorch experiment results\n",
      "  ├── 📄 experiment_config.yaml - Experiment configuration\n",
      "  └── 📄 *.png - Various performance plots\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive Results Analysis with All Metrics\n",
    "logger.info(\"Starting comprehensive analysis and plotting of all metrics\")\n",
    "\n",
    "def create_comprehensive_performance_plots(df, model_type):\n",
    "    \"\"\"Create comprehensive performance plots for all metrics\"\"\"\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    if 'hidden_layer_sizes' in df.columns:\n",
    "        df['arch_str'] = df['hidden_layer_sizes'].apply(lambda x: str(x).replace(' ', ''))\n",
    "        x_col = 'arch_str'\n",
    "        hue_col = 'solver'\n",
    "    else:\n",
    "        df['arch_str'] = df['hidden_sizes'].apply(lambda x: str(x).replace(' ', ''))\n",
    "        x_col = 'arch_str'\n",
    "        hue_col = 'dropout'\n",
    "    \n",
    "    # Create a 2x2 subplot for different metrics\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 15))\n",
    "    fig.suptitle(f'{model_type} - Comprehensive Performance Analysis', fontsize=16)\n",
    "    \n",
    "    # Plot 1: Accuracy\n",
    "    sns.barplot(x=x_col, y='test_acc', hue=hue_col, data=df, ax=axes[0,0], palette='viridis')\n",
    "    axes[0,0].set_title('Test Accuracy')\n",
    "    axes[0,0].set_xlabel('Architecture')\n",
    "    axes[0,0].set_ylabel('Accuracy')\n",
    "    axes[0,0].tick_params(axis='x', rotation=45)\n",
    "    axes[0,0].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Plot 2: F1-Score (Macro)\n",
    "    sns.barplot(x=x_col, y='test_f1_macro', hue=hue_col, data=df, ax=axes[0,1], palette='plasma')\n",
    "    axes[0,1].set_title('Test F1-Score (Macro)')\n",
    "    axes[0,1].set_xlabel('Architecture')\n",
    "    axes[0,1].set_ylabel('F1-Score')\n",
    "    axes[0,1].tick_params(axis='x', rotation=45)\n",
    "    axes[0,1].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Plot 3: Precision (Macro)\n",
    "    sns.barplot(x=x_col, y='test_precision_macro', hue=hue_col, data=df, ax=axes[1,0], palette='cividis')\n",
    "    axes[1,0].set_title('Test Precision (Macro)')\n",
    "    axes[1,0].set_xlabel('Architecture')\n",
    "    axes[1,0].set_ylabel('Precision')\n",
    "    axes[1,0].tick_params(axis='x', rotation=45)\n",
    "    axes[1,0].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Plot 4: Recall (Macro)\n",
    "    sns.barplot(x=x_col, y='test_recall_macro', hue=hue_col, data=df, ax=axes[1,1], palette='magma')\n",
    "    axes[1,1].set_title('Test Recall (Macro)')\n",
    "    axes[1,1].set_xlabel('Architecture')\n",
    "    axes[1,1].set_ylabel('Recall')\n",
    "    axes[1,1].tick_params(axis='x', rotation=45)\n",
    "    axes[1,1].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    filename = f'{model_type.lower()}_comprehensive_metrics.png'\n",
    "    plt.savefig(os.path.join(OUTDIR, filename), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    logger.info(f\"Saved comprehensive metrics plot: {filename}\")\n",
    "\n",
    "def create_cv_comparison_plots(df, model_type):\n",
    "    \"\"\"Create cross-validation comparison plots if CV was used\"\"\"\n",
    "    if not use_cross_validation:\n",
    "        return\n",
    "    \n",
    "    cv_col = 'cv_mean_acc' if model_type == 'Scikit-learn' else 'cv_val_mean'\n",
    "    if cv_col not in df.columns:\n",
    "        return\n",
    "    \n",
    "    # Prepare data for plotting\n",
    "    if 'hidden_layer_sizes' in df.columns:\n",
    "        df['arch_str'] = df['hidden_layer_sizes'].apply(lambda x: str(x).replace(' ', ''))\n",
    "        x_col = 'arch_str'\n",
    "        hue_col = 'solver'\n",
    "    else:\n",
    "        df['arch_str'] = df['hidden_sizes'].apply(lambda x: str(x).replace(' ', ''))\n",
    "        x_col = 'arch_str'\n",
    "        hue_col = 'dropout'\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # CV results with error bars\n",
    "    cv_std_col = cv_col.replace('mean', 'std')\n",
    "    if cv_std_col in df.columns:\n",
    "        # Group data for error bars\n",
    "        grouped = df.groupby([x_col, hue_col]).agg({\n",
    "            cv_col: 'mean',\n",
    "            cv_std_col: 'mean'\n",
    "        }).reset_index()\n",
    "        \n",
    "        sns.barplot(x=x_col, y=cv_col, hue=hue_col, data=grouped, ax=axes[0], palette='viridis')\n",
    "        \n",
    "        # Add error bars\n",
    "        for i, (_, row) in enumerate(grouped.iterrows()):\n",
    "            axes[0].errorbar(i, row[cv_col], yerr=row[cv_std_col], \n",
    "                           fmt='none', color='black', capsize=3, alpha=0.7)\n",
    "        \n",
    "        axes[0].set_title(f'{model_type} - Cross-Validation Results')\n",
    "        axes[0].set_xlabel('Architecture')\n",
    "        axes[0].set_ylabel('CV Accuracy')\n",
    "        axes[0].tick_params(axis='x', rotation=45)\n",
    "        axes[0].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # CV vs Test correlation\n",
    "    axes[1].scatter(df[cv_col], df['test_acc'], alpha=0.7)\n",
    "    axes[1].plot([0, 1], [0, 1], 'r--', alpha=0.8)\n",
    "    correlation = df[cv_col].corr(df['test_acc'])\n",
    "    axes[1].set_xlabel('Cross-Validation Accuracy')\n",
    "    axes[1].set_ylabel('Test Set Accuracy')\n",
    "    axes[1].set_title(f'{model_type} CV vs Test Correlation (r={correlation:.3f})')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    filename = f'{model_type.lower()}_cv_analysis.png'\n",
    "    plt.savefig(os.path.join(OUTDIR, filename), dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    logger.info(f\"Saved CV analysis plot: {filename}\")\n",
    "\n",
    "# Create comprehensive plots for both model types\n",
    "if 'sklearn_df' in locals() and not sklearn_df.empty:\n",
    "    create_comprehensive_performance_plots(sklearn_df, 'Scikit-learn')\n",
    "    create_cv_comparison_plots(sklearn_df, 'Scikit-learn')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SCIKIT-LEARN COMPREHENSIVE RESULTS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total experiments: {len(sklearn_df)}\")\n",
    "    \n",
    "    # Best results summary\n",
    "    metrics_to_show = ['test_acc', 'test_f1_macro', 'test_precision_macro', 'test_recall_macro']\n",
    "    if use_cross_validation:\n",
    "        cv_best = sklearn_df.loc[sklearn_df['cv_mean_acc'].idxmax()]\n",
    "        print(f\"\\nBest model (by CV accuracy):\")\n",
    "        print(f\"  CV accuracy: {cv_best['cv_mean_acc']:.4f}±{cv_best['cv_std_acc']:.4f}\")\n",
    "        for metric in metrics_to_show:\n",
    "            print(f\"  {metric}: {cv_best[metric]:.4f}\")\n",
    "        print(f\"  Parameters: {cv_best['hidden_layer_sizes']}, {cv_best['solver']}, lr={cv_best['learning_rate_init']}\")\n",
    "    \n",
    "    # Overall statistics\n",
    "    print(f\"\\nOverall Performance Statistics:\")\n",
    "    for metric in metrics_to_show:\n",
    "        print(f\"  {metric}: {sklearn_df[metric].mean():.4f}±{sklearn_df[metric].std():.4f} (best: {sklearn_df[metric].max():.4f})\")\n",
    "\n",
    "if 'pytorch_df' in locals() and not pytorch_df.empty:\n",
    "    create_comprehensive_performance_plots(pytorch_df, 'PyTorch')\n",
    "    create_cv_comparison_plots(pytorch_df, 'PyTorch')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PYTORCH COMPREHENSIVE RESULTS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total experiments: {len(pytorch_df)}\")\n",
    "    \n",
    "    # Best results summary\n",
    "    metrics_to_show = ['test_acc', 'test_f1_macro', 'test_precision_macro', 'test_recall_macro']\n",
    "    if use_cross_validation:\n",
    "        cv_best = pytorch_df.loc[pytorch_df['cv_val_mean'].idxmax()]\n",
    "        print(f\"\\nBest model (by CV accuracy):\")\n",
    "        print(f\"  CV accuracy: {cv_best['cv_val_mean']:.4f}±{cv_best['cv_val_std']:.4f}\")\n",
    "        for metric in metrics_to_show:\n",
    "            print(f\"  {metric}: {cv_best[metric]:.4f}\")\n",
    "        print(f\"  Parameters: {cv_best['hidden_sizes']}, lr={cv_best['learning_rate']}, dropout={cv_best['dropout']}\")\n",
    "    \n",
    "    # Overall statistics  \n",
    "    print(f\"\\nOverall Performance Statistics:\")\n",
    "    for metric in metrics_to_show:\n",
    "        print(f\"  {metric}: {pytorch_df[metric].mean():.4f}±{pytorch_df[metric].std():.4f} (best: {pytorch_df[metric].max():.4f})\")\n",
    "\n",
    "# Model Comparison (if both models were run)\n",
    "if 'sklearn_df' in locals() and 'pytorch_df' in locals() and not sklearn_df.empty and not pytorch_df.empty:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"MODEL COMPARISON SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    comparison_metrics = ['test_acc', 'test_f1_macro', 'test_precision_macro', 'test_recall_macro']\n",
    "    \n",
    "    print(\"Best Performance Comparison:\")\n",
    "    for metric in comparison_metrics:\n",
    "        sklearn_best = sklearn_df[metric].max()\n",
    "        pytorch_best = pytorch_df[metric].max()\n",
    "        winner = \"Scikit-learn\" if sklearn_best > pytorch_best else \"PyTorch\"\n",
    "        print(f\"  {metric}: Sklearn={sklearn_best:.4f}, PyTorch={pytorch_best:.4f} → Winner: {winner}\")\n",
    "    \n",
    "    print(\"\\nAverage Performance Comparison:\")\n",
    "    for metric in comparison_metrics:\n",
    "        sklearn_avg = sklearn_df[metric].mean()\n",
    "        pytorch_avg = pytorch_df[metric].mean()\n",
    "        winner = \"Scikit-learn\" if sklearn_avg > pytorch_avg else \"PyTorch\"\n",
    "        print(f\"  {metric}: Sklearn={sklearn_avg:.4f}, PyTorch={pytorch_avg:.4f} → Winner: {winner}\")\n",
    "\n",
    "# Training time analysis\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING TIME AND COMPUTATIONAL ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'sklearn_df' in locals() and not sklearn_df.empty:\n",
    "    sklearn_time = sklearn_df['duration_seconds']\n",
    "    print(f\"Scikit-learn:\")\n",
    "    print(f\"  Average time per experiment: {sklearn_time.mean():.2f}s\")\n",
    "    print(f\"  Total time: {sklearn_time.sum():.2f}s ({sklearn_time.sum()/60:.1f} minutes)\")\n",
    "\n",
    "if 'pytorch_df' in locals() and not pytorch_df.empty:\n",
    "    pytorch_time = pytorch_df['duration_seconds']\n",
    "    print(f\"PyTorch:\")\n",
    "    print(f\"  Average time per experiment: {pytorch_time.mean():.2f}s\")\n",
    "    print(f\"  Total time: {pytorch_time.sum():.2f}s ({pytorch_time.sum()/60:.1f} minutes)\")\n",
    "\n",
    "if use_cross_validation:\n",
    "    print(f\"\\nCross-validation impact:\")\n",
    "    print(f\"  Estimated {n_folds}x time increase due to CV\")\n",
    "    if 'sklearn_df' in locals():\n",
    "        print(f\"  Sklearn estimated time without CV: {sklearn_df['duration_seconds'].mean() / n_folds:.2f}s per experiment\")\n",
    "    if 'pytorch_df' in locals():\n",
    "        print(f\"  PyTorch estimated time without CV: {pytorch_df['duration_seconds'].mean() / n_folds:.2f}s per experiment\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FILES AND OUTPUTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(\"Generated files and directories:\")\n",
    "print(\"📁 experiment_outputs/\")\n",
    "print(\"  ├── 📁 sklearn_models/ - Individual sklearn models (.joblib)\")\n",
    "print(\"  ├── 📁 pytorch_models/ - Individual PyTorch models (.pth)\")\n",
    "print(\"  ├── 📁 best_models/ - Best models from each framework\")\n",
    "print(\"  ├── 📁 metrics/ - Comprehensive metrics for each model (.json)\")\n",
    "print(\"  ├── 📁 confusion_matrices/ - Confusion matrix plots (.png)\")\n",
    "print(\"  ├── 📁 tb_logs/ - TensorBoard logs (PyTorch only)\")\n",
    "print(\"  ├── 📄 sklearn_results_summary.csv - Sklearn experiment results\")\n",
    "print(\"  ├── 📄 pytorch_results_summary.csv - PyTorch experiment results\")\n",
    "print(\"  ├── 📄 experiment_config.yaml - Experiment configuration\")\n",
    "print(\"  └── 📄 *.png - Various performance plots\")\n",
    "\n",
    "logger.info(\"Comprehensive analysis complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "721a00cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 00:23:41,565 - INFO - Fixed deployment package created successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 CREATING FIXED DEPLOYMENT PACKAGE...\n",
      "============================================================\n",
      "📊 FEATURE INFORMATION:\n",
      "   Selected features for training: 35\n",
      "   Feature names: ['has_dental_data', 'special_needs', 'caregiver_treatment', 'plaque', 'dry_mouth', 'enamel_defects', 'fluoride_water', 'fluoride_toothpaste', 'topical_fluoride', 'regular_checkups', 'sealed_pits', 'enamel_change', 'dentin_discoloration', 'white_spot_lesions', 'cavitated_lesions', 'multiple_restorations', 'missing_teeth', 'total_dmft_score', 'sweet_sugary_foods', 'sweet_sugary_foods_bedtime', 'takeaways_processed_foods', 'fresh_fruit', 'cold_drinks_juices', 'processed_fruit', 'processed_fruit_bedtime', 'spreads', 'spreads_bedtime', 'added_sugars', 'added_sugars_bedtime', 'salty_snacks', 'dairy_products', 'vegetables', 'has_dietary_data', 'cold_drinks_juices_bedtime', 'water']\n",
      "✅ Feature names saved to: experiment_outputs\\best_models\\feature_names.pkl\n",
      "\n",
      "🔧 CREATING CORRECT SCALER...\n",
      "   Original data shape: (25000, 69)\n",
      "   Selected features shape: (25000, 35)\n",
      "   Scaler fitted on: 35 features ✅\n",
      "   Expected input shape: 35\n",
      "✅ CORRECT scaler saved to: experiment_outputs\\best_models\\scaler.pkl\n",
      "✅ CORRECT scaler (joblib) saved to: experiment_outputs\\best_models\\scaler.joblib\n",
      "✅ Feature information saved to: experiment_outputs\\best_models\\feature_info.json\n",
      "✅ Updated PyTorch best model info with correct scaler paths\n",
      "✅ Deployment summary saved to: experiment_outputs\\best_models\\deployment_summary.json\n",
      "\n",
      "🧪 TESTING DEPLOYMENT PACKAGE...\n",
      "✅ Test: Feature names loaded (35 features)\n",
      "✅ Test: Scaler loaded (expects 35 features)\n",
      "✅ Test: Sample data transformation successful\n",
      "   Input shape: (1, 35)\n",
      "   Output shape: (1, 35)\n",
      "\n",
      "📦 DEPLOYMENT PACKAGE READY:\n",
      "Location: experiment_outputs\\best_models\n",
      "Files created:\n",
      "  ├── feature_names.pkl (List of 35 feature names - REQUIRED)\n",
      "  ├── scaler.pkl (StandardScaler fitted on 35 features)\n",
      "  ├── scaler.joblib (Same scaler in joblib format)\n",
      "  ├── feature_info.json (Complete feature metadata)\n",
      "  ├── deployment_summary.json (Usage instructions)\n",
      "  └── Model files (best_pytorch_model.pth or best_sklearn_model.joblib)\n",
      "\n",
      "⚠️  CRITICAL FOR YOUR APP:\n",
      "   1. Your app data MUST have exactly 35 features\n",
      "   2. Feature names must match exactly: ['has_dental_data', 'special_needs', 'caregiver_treatment', 'plaque', 'dry_mouth', 'enamel_defects', 'fluoride_water', 'fluoride_toothpaste', 'topical_fluoride', 'regular_checkups', 'sealed_pits', 'enamel_change', 'dentin_discoloration', 'white_spot_lesions', 'cavitated_lesions', 'multiple_restorations', 'missing_teeth', 'total_dmft_score', 'sweet_sugary_foods', 'sweet_sugary_foods_bedtime', 'takeaways_processed_foods', 'fresh_fruit', 'cold_drinks_juices', 'processed_fruit', 'processed_fruit_bedtime', 'spreads', 'spreads_bedtime', 'added_sugars', 'added_sugars_bedtime', 'salty_snacks', 'dairy_products', 'vegetables', 'has_dietary_data', 'cold_drinks_juices_bedtime', 'water']\n",
      "   3. Use the scaler.pkl file (fitted on 35 features)\n",
      "   4. Apply feature selection BEFORE scaling in your app\n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# DEPLOYMENT PACKAGE CREATION (FIXED)\n",
    "# ======================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create deployment directory\n",
    "best_model_dir = os.path.join(OUTDIR, 'best_models')\n",
    "os.makedirs(best_model_dir, exist_ok=True)\n",
    "\n",
    "print(\"🚀 CREATING FIXED DEPLOYMENT PACKAGE...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Extract feature names for app deployment\n",
    "feature_names = list(X.columns)  # The selected features used in training (in correct order)\n",
    "n_features = len(X.columns)\n",
    "\n",
    "print(f\"📊 FEATURE INFORMATION:\")\n",
    "print(f\"   Selected features for training: {n_features}\")\n",
    "print(f\"   Feature names: {feature_names}\")\n",
    "\n",
    "# Save feature names as pickle/joblib file (for app requirements)\n",
    "feature_names_path = os.path.join(best_model_dir, 'feature_names.pkl')\n",
    "joblib.dump(feature_names, feature_names_path)\n",
    "\n",
    "print(f\"✅ Feature names saved to: {feature_names_path}\")\n",
    "\n",
    "# ⚠️ CRITICAL FIX: Create scaler fitted ONLY on selected features\n",
    "print(f\"\\n🔧 CREATING CORRECT SCALER...\")\n",
    "print(f\"   Original data shape: {df.shape}\")\n",
    "print(f\"   Selected features shape: {X.shape}\")\n",
    "\n",
    "# Create and fit scaler on the SAME selected features used for training\n",
    "correct_scaler = StandardScaler()\n",
    "X_selected_features = df[feature_names]  # Select features from original data\n",
    "correct_scaler.fit(X_selected_features)\n",
    "\n",
    "print(f\"   Scaler fitted on: {correct_scaler.n_features_in_} features ✅\")\n",
    "print(f\"   Expected input shape: {correct_scaler.n_features_in_}\")\n",
    "\n",
    "# Save the CORRECT scaler\n",
    "scaler_path = os.path.join(best_model_dir, 'scaler.pkl')\n",
    "joblib.dump(correct_scaler, scaler_path)\n",
    "\n",
    "print(f\"✅ CORRECT scaler saved to: {scaler_path}\")\n",
    "\n",
    "# Also save as joblib format\n",
    "scaler_joblib_path = os.path.join(best_model_dir, 'scaler.joblib')\n",
    "joblib.dump(correct_scaler, scaler_joblib_path)\n",
    "\n",
    "print(f\"✅ CORRECT scaler (joblib) saved to: {scaler_joblib_path}\")\n",
    "\n",
    "# Feature names and preprocessing information (comprehensive metadata)\n",
    "feature_info = {\n",
    "    'feature_names': feature_names,\n",
    "    'n_features': n_features,\n",
    "    'feature_selection_info': {\n",
    "        'feature_selection_applied': True,\n",
    "        'selected_features': selected_features,  # Original feature selection list\n",
    "        'available_features': available_features,  # Features that were actually available\n",
    "        'missing_features': missing_features if 'missing_features' in locals() else [],\n",
    "        'total_original_features': len(selected_features)\n",
    "    },\n",
    "    'preprocessing_info': {\n",
    "        'scaler_type': 'StandardScaler',\n",
    "        'scaling_applied': True,\n",
    "        'scaler_fitted_on_features': n_features,  # CRITICAL: scaler expects exactly these features\n",
    "        'target_column': target_col,\n",
    "        'data_split_info': {\n",
    "            'train_size': X_train.shape[0],\n",
    "            'test_size': X_test.shape[0],\n",
    "            'train_ratio': X_train.shape[0] / (X_train.shape[0] + X_test.shape[0])\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save feature information\n",
    "feature_info_path = os.path.join(best_model_dir, 'feature_info.json')\n",
    "with open(feature_info_path, 'w') as f:\n",
    "    json.dump(feature_info, f, indent=2)\n",
    "\n",
    "print(f\"✅ Feature information saved to: {feature_info_path}\")\n",
    "\n",
    "# Update best model info files with correct scaler paths\n",
    "if 'best_pytorch_model' in locals() and best_pytorch_model:\n",
    "    # Update PyTorch model info\n",
    "    pytorch_info_path = os.path.join(best_model_dir, 'best_pytorch_model_info.json')\n",
    "    if os.path.exists(pytorch_info_path):\n",
    "        with open(pytorch_info_path, 'r') as f:\n",
    "            pytorch_best_info = json.load(f)\n",
    "        \n",
    "        # Add scaler and feature info\n",
    "        pytorch_best_info['deployment_files'] = {\n",
    "            'feature_names_pkl': feature_names_path,\n",
    "            'scaler_pkl': scaler_path,\n",
    "            'scaler_joblib': scaler_joblib_path,\n",
    "            'feature_info_json': feature_info_path\n",
    "        }\n",
    "        \n",
    "        with open(pytorch_info_path, 'w') as f:\n",
    "            json.dump(pytorch_best_info, f, indent=2)\n",
    "        \n",
    "        print(f\"✅ Updated PyTorch best model info with correct scaler paths\")\n",
    "\n",
    "# Create deployment summary\n",
    "deployment_summary = {\n",
    "    'deployment_ready': True,\n",
    "    'model_type': 'PyTorch' if 'best_pytorch_model' in locals() and best_pytorch_model else 'Scikit-learn',\n",
    "    'feature_count': n_features,\n",
    "    'scaler_input_features': correct_scaler.n_features_in_,\n",
    "    'files': {\n",
    "        'feature_names': 'feature_names.pkl',\n",
    "        'scaler': 'scaler.pkl',\n",
    "        'scaler_joblib': 'scaler.joblib', \n",
    "        'feature_info': 'feature_info.json',\n",
    "        'model': 'best_pytorch_model.pth' if 'best_pytorch_model' in locals() and best_pytorch_model else 'best_sklearn_model.joblib'\n",
    "    },\n",
    "    'usage_instructions': {\n",
    "        'step_1': 'Load feature_names.pkl to get the exact feature list',\n",
    "        'step_2': 'Ensure your input data has exactly these features in this order',\n",
    "        'step_3': 'Load scaler.pkl and apply: X_scaled = scaler.transform(X)',\n",
    "        'step_4': 'Load model and predict: predictions = model.predict(X_scaled)',\n",
    "        'critical_note': f'Input data MUST have exactly {n_features} features matching the feature_names list'\n",
    "    }\n",
    "}\n",
    "\n",
    "deployment_summary_path = os.path.join(best_model_dir, 'deployment_summary.json')\n",
    "with open(deployment_summary_path, 'w') as f:\n",
    "    json.dump(deployment_summary, f, indent=2)\n",
    "\n",
    "print(f\"✅ Deployment summary saved to: {deployment_summary_path}\")\n",
    "\n",
    "# Test the deployment package\n",
    "print(f\"\\n🧪 TESTING DEPLOYMENT PACKAGE...\")\n",
    "try:\n",
    "    # Test loading all files\n",
    "    test_feature_names = joblib.load(feature_names_path)\n",
    "    test_scaler = joblib.load(scaler_path)\n",
    "    \n",
    "    print(f\"✅ Test: Feature names loaded ({len(test_feature_names)} features)\")\n",
    "    print(f\"✅ Test: Scaler loaded (expects {test_scaler.n_features_in_} features)\")\n",
    "    \n",
    "    # Test with sample data\n",
    "    sample_data = X.iloc[:1].copy()  # Take one row\n",
    "    sample_scaled = test_scaler.transform(sample_data)\n",
    "    \n",
    "    print(f\"✅ Test: Sample data transformation successful\")\n",
    "    print(f\"   Input shape: {sample_data.shape}\")\n",
    "    print(f\"   Output shape: {sample_scaled.shape}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Test failed: {e}\")\n",
    "\n",
    "print(f\"\\n📦 DEPLOYMENT PACKAGE READY:\")\n",
    "print(f\"Location: {best_model_dir}\")\n",
    "print(f\"Files created:\")\n",
    "print(f\"  ├── feature_names.pkl (List of {n_features} feature names - REQUIRED)\")\n",
    "print(f\"  ├── scaler.pkl (StandardScaler fitted on {n_features} features)\")  \n",
    "print(f\"  ├── scaler.joblib (Same scaler in joblib format)\")\n",
    "print(f\"  ├── feature_info.json (Complete feature metadata)\")\n",
    "print(f\"  ├── deployment_summary.json (Usage instructions)\")\n",
    "print(f\"  └── Model files (best_pytorch_model.pth or best_sklearn_model.joblib)\")\n",
    "\n",
    "print(f\"\\n⚠️  CRITICAL FOR YOUR APP:\")\n",
    "print(f\"   1. Your app data MUST have exactly {n_features} features\")\n",
    "print(f\"   2. Feature names must match exactly: {feature_names}\")\n",
    "print(f\"   3. Use the scaler.pkl file (fitted on {n_features} features)\")\n",
    "print(f\"   4. Apply feature selection BEFORE scaling in your app\")\n",
    "\n",
    "logger.info(\"Fixed deployment package created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0728ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-06 01:10:33,257 - INFO - Comprehensive deployment package created with multiple model formats!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 CREATING COMPREHENSIVE DEPLOYMENT PACKAGE\n",
      "======================================================================\n",
      "📦 SAVING MODEL IN MULTIPLE FORMATS:\n",
      "✅ PyTorch model saved as pickle: best_pytorch_model.pkl\n",
      "✅ PyTorch model saved as pickle: best_pytorch_model.pickle\n",
      "✅ PyTorch model saved as joblib: best_pytorch_model.joblib\n",
      "✅ PyTorch model available as: best_pytorch_model.pth\n",
      "\n",
      "🔧 MODEL ARCHITECTURE & PARAMETERS:\n",
      "   Hidden layers: [64]\n",
      "   Input features: 35\n",
      "   Output classes: 3\n",
      "   Dropout: 0.2\n",
      "   Learning rate: 0.001\n",
      "\n",
      "📊 MODEL PERFORMANCE:\n",
      "   Test Accuracy: 0.9930\n",
      "   F1-Score (macro): 0.9930\n",
      "   Precision (macro): 0.9934\n",
      "   Recall (macro): 0.9927\n",
      "✅ Model wrapper class saved: model_wrapper.py\n",
      "✅ Deployment instructions saved: deployment_instructions.json\n",
      "\n",
      "📁 FINAL DEPLOYMENT PACKAGE:\n",
      "├── 🤖 MODEL FILES (Choose one):\n",
      "│   ├── best_pytorch_model.pkl     (Joblib pickle - RECOMMENDED)\n",
      "│   ├── best_pytorch_model.pickle  (Standard pickle)\n",
      "│   ├── best_pytorch_model.joblib  (Joblib format)\n",
      "│   └── best_pytorch_model.pth     (PyTorch format)\n",
      "├── 🔧 PREPROCESSING:\n",
      "│   ├── feature_names.pkl          (35 feature names)\n",
      "│   ├── scaler.pkl                 (StandardScaler)\n",
      "│   └── feature_info.json          (Feature metadata)\n",
      "├── 📝 DEPLOYMENT TOOLS:\n",
      "│   ├── model_wrapper.py           (Easy-to-use wrapper class)\n",
      "│   └── deployment_instructions.json (Complete usage guide)\n",
      "└── 📊 DOCUMENTATION:\n",
      "    └── deployment_summary.json    (Summary and metadata)\n",
      "\n",
      "🎯 RECOMMENDED USAGE:\n",
      "   1. Use: best_pytorch_model.pkl (easiest to load)\n",
      "   2. Use: model_wrapper.py (handles everything automatically)\n",
      "   3. Example: model = OralHealthMLPWrapper('best_pytorch_model.pkl', 'scaler.pkl', 'feature_names.pkl')\n",
      "\n",
      "🧪 TESTING PICKLE MODEL LOADING:\n",
      "✅ Pickle model loads successfully\n",
      "   Model type: <class 'dict'>\n",
      "   Contains keys: ['model_state_dict', 'params', 'test_metrics', 'run']\n"
     ]
    }
   ],
   "source": [
    "# =======================================\n",
    "# COMPREHENSIVE MODEL DEPLOYMENT PACKAGE\n",
    "# =======================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "import joblib\n",
    "import torch\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"🚀 CREATING COMPREHENSIVE DEPLOYMENT PACKAGE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Load the best PyTorch model\n",
    "pytorch_model_path = os.path.join(best_model_dir, 'best_pytorch_model.pth')\n",
    "pytorch_model_dict = torch.load(pytorch_model_path, map_location='cpu')\n",
    "\n",
    "print(\"📦 SAVING MODEL IN MULTIPLE FORMATS:\")\n",
    "\n",
    "# 1. Save as Pickle (.pkl)\n",
    "pytorch_pkl_path = os.path.join(best_model_dir, 'best_pytorch_model.pkl')\n",
    "joblib.dump(pytorch_model_dict, pytorch_pkl_path)\n",
    "print(f\"✅ PyTorch model saved as pickle: best_pytorch_model.pkl\")\n",
    "\n",
    "# 2. Save using standard pickle\n",
    "pytorch_pickle_path = os.path.join(best_model_dir, 'best_pytorch_model.pickle')\n",
    "with open(pytorch_pickle_path, 'wb') as f:\n",
    "    pickle.dump(pytorch_model_dict, f)\n",
    "print(f\"✅ PyTorch model saved as pickle: best_pytorch_model.pickle\")\n",
    "\n",
    "# 3. Save as Joblib\n",
    "pytorch_joblib_path = os.path.join(best_model_dir, 'best_pytorch_model.joblib')\n",
    "joblib.dump(pytorch_model_dict, pytorch_joblib_path)\n",
    "print(f\"✅ PyTorch model saved as joblib: best_pytorch_model.joblib\")\n",
    "\n",
    "# 4. Keep original .pth format\n",
    "print(f\"✅ PyTorch model available as: best_pytorch_model.pth\")\n",
    "\n",
    "print(f\"\\n🔧 MODEL ARCHITECTURE & PARAMETERS:\")\n",
    "print(f\"   Hidden layers: {pytorch_model_dict['params']['hidden_sizes']}\")\n",
    "print(f\"   Input features: 35\")\n",
    "print(f\"   Output classes: 3\")\n",
    "print(f\"   Dropout: {pytorch_model_dict['params']['dropout']}\")\n",
    "print(f\"   Learning rate: {pytorch_model_dict['params']['learning_rate']}\")\n",
    "\n",
    "print(f\"\\n📊 MODEL PERFORMANCE:\")\n",
    "metrics = pytorch_model_dict['test_metrics']\n",
    "print(f\"   Test Accuracy: {metrics['accuracy']:.4f}\")\n",
    "print(f\"   F1-Score (macro): {metrics['f1_macro']:.4f}\")\n",
    "print(f\"   Precision (macro): {metrics['precision_macro']:.4f}\")\n",
    "print(f\"   Recall (macro): {metrics['recall_macro']:.4f}\")\n",
    "\n",
    "# Create a simple Python class for easy model loading and prediction\n",
    "model_wrapper_code = '''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "class OralHealthMLPWrapper:\n",
    "    \"\"\"\n",
    "    Wrapper class for the OralHealth MLP model for easy deployment.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model_path, scaler_path, feature_names_path):\n",
    "        \"\"\"\n",
    "        Initialize the model wrapper.\n",
    "        \n",
    "        Args:\n",
    "            model_path: Path to the saved model (.pkl, .pickle, .joblib, or .pth)\n",
    "            scaler_path: Path to the saved scaler (.pkl or .joblib)\n",
    "            feature_names_path: Path to the saved feature names (.pkl)\n",
    "        \"\"\"\n",
    "        # Load feature names\n",
    "        self.feature_names = joblib.load(feature_names_path)\n",
    "        self.n_features = len(self.feature_names)\n",
    "        \n",
    "        # Load scaler\n",
    "        self.scaler = joblib.load(scaler_path)\n",
    "        \n",
    "        # Load model\n",
    "        if model_path.endswith('.pth'):\n",
    "            self.model_dict = torch.load(model_path, map_location='cpu')\n",
    "        else:\n",
    "            self.model_dict = joblib.load(model_path)\n",
    "        \n",
    "        # Recreate the model architecture\n",
    "        self.model = self._create_model()\n",
    "        self.model.load_state_dict(self.model_dict['model_state_dict'])\n",
    "        self.model.eval()\n",
    "        \n",
    "        # Class names\n",
    "        self.class_names = ['Low Risk', 'Medium Risk', 'High Risk']\n",
    "        \n",
    "    def _create_model(self):\n",
    "        \"\"\"Recreate the model architecture.\"\"\"\n",
    "        hidden_sizes = self.model_dict['params']['hidden_sizes']\n",
    "        dropout = self.model_dict['params']['dropout']\n",
    "        \n",
    "        layers = []\n",
    "        input_size = self.n_features\n",
    "        \n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.extend([\n",
    "                nn.Linear(input_size, hidden_size),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout)\n",
    "            ])\n",
    "            input_size = hidden_size\n",
    "        \n",
    "        # Output layer\n",
    "        layers.append(nn.Linear(input_size, 3))  # 3 classes\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on input data.\n",
    "        \n",
    "        Args:\n",
    "            X: Input data (pandas DataFrame or numpy array)\n",
    "            \n",
    "        Returns:\n",
    "            predictions: Array of predicted class indices\n",
    "        \"\"\"\n",
    "        # Ensure X has the correct features\n",
    "        if hasattr(X, 'columns'):\n",
    "            # DataFrame - select and reorder features\n",
    "            X = X[self.feature_names]\n",
    "        \n",
    "        # Convert to numpy if needed\n",
    "        if hasattr(X, 'values'):\n",
    "            X = X.values\n",
    "        \n",
    "        # Scale the data\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        \n",
    "        # Convert to tensor\n",
    "        X_tensor = torch.FloatTensor(X_scaled)\n",
    "        \n",
    "        # Make predictions\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(X_tensor)\n",
    "            predictions = torch.argmax(outputs, dim=1)\n",
    "        \n",
    "        return predictions.numpy()\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Get prediction probabilities.\n",
    "        \n",
    "        Args:\n",
    "            X: Input data (pandas DataFrame or numpy array)\n",
    "            \n",
    "        Returns:\n",
    "            probabilities: Array of prediction probabilities\n",
    "        \"\"\"\n",
    "        # Ensure X has the correct features\n",
    "        if hasattr(X, 'columns'):\n",
    "            X = X[self.feature_names]\n",
    "        \n",
    "        if hasattr(X, 'values'):\n",
    "            X = X.values\n",
    "        \n",
    "        # Scale the data\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        X_tensor = torch.FloatTensor(X_scaled)\n",
    "        \n",
    "        # Get probabilities\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(X_tensor)\n",
    "            probabilities = torch.softmax(outputs, dim=1)\n",
    "        \n",
    "        return probabilities.numpy()\n",
    "    \n",
    "    def predict_with_labels(self, X):\n",
    "        \"\"\"\n",
    "        Get predictions with class labels.\n",
    "        \n",
    "        Args:\n",
    "            X: Input data\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (predictions, class_labels, probabilities)\n",
    "        \"\"\"\n",
    "        pred_indices = self.predict(X)\n",
    "        pred_labels = [self.class_names[i] for i in pred_indices]\n",
    "        probabilities = self.predict_proba(X)\n",
    "        \n",
    "        return pred_indices, pred_labels, probabilities\n",
    "\n",
    "# Usage example:\n",
    "# model = OralHealthMLPWrapper(\n",
    "#     model_path='best_pytorch_model.pkl',\n",
    "#     scaler_path='scaler.pkl', \n",
    "#     feature_names_path='feature_names.pkl'\n",
    "# )\n",
    "# predictions = model.predict(your_data)\n",
    "# pred_indices, pred_labels, probabilities = model.predict_with_labels(your_data)\n",
    "'''\n",
    "\n",
    "# Save the wrapper class code\n",
    "wrapper_path = os.path.join(best_model_dir, 'model_wrapper.py')\n",
    "with open(wrapper_path, 'w') as f:\n",
    "    f.write(model_wrapper_code)\n",
    "\n",
    "print(f\"✅ Model wrapper class saved: model_wrapper.py\")\n",
    "\n",
    "# Create deployment instructions\n",
    "deployment_instructions = {\n",
    "    \"deployment_ready\": True,\n",
    "    \"model_formats\": {\n",
    "        \"pytorch_pth\": \"best_pytorch_model.pth (PyTorch format)\",\n",
    "        \"pickle_pkl\": \"best_pytorch_model.pkl (Joblib pickle)\",\n",
    "        \"pickle_std\": \"best_pytorch_model.pickle (Standard pickle)\", \n",
    "        \"joblib\": \"best_pytorch_model.joblib (Joblib format)\"\n",
    "    },\n",
    "    \"preprocessing\": {\n",
    "        \"feature_names\": \"feature_names.pkl (35 features)\",\n",
    "        \"scaler\": \"scaler.pkl (StandardScaler fitted on 35 features)\"\n",
    "    },\n",
    "    \"model_info\": {\n",
    "        \"input_features\": 35,\n",
    "        \"output_classes\": 3,\n",
    "        \"class_names\": [\"Low Risk\", \"Medium Risk\", \"High Risk\"],\n",
    "        \"architecture\": pytorch_model_dict['params']\n",
    "    },\n",
    "    \"usage_options\": {\n",
    "        \"option_1_simple\": {\n",
    "            \"description\": \"Load individual files manually\",\n",
    "            \"steps\": [\n",
    "                \"feature_names = joblib.load('feature_names.pkl')\",\n",
    "                \"scaler = joblib.load('scaler.pkl')\",\n",
    "                \"model_dict = joblib.load('best_pytorch_model.pkl')\",\n",
    "                \"# Apply preprocessing and prediction manually\"\n",
    "            ]\n",
    "        },\n",
    "        \"option_2_wrapper\": {\n",
    "            \"description\": \"Use the provided wrapper class (RECOMMENDED)\",\n",
    "            \"steps\": [\n",
    "                \"from model_wrapper import OralHealthMLPWrapper\",\n",
    "                \"model = OralHealthMLPWrapper('best_pytorch_model.pkl', 'scaler.pkl', 'feature_names.pkl')\",\n",
    "                \"predictions = model.predict(your_data)\",\n",
    "                \"pred_indices, pred_labels, probabilities = model.predict_with_labels(your_data)\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "instructions_path = os.path.join(best_model_dir, 'deployment_instructions.json')\n",
    "with open(instructions_path, 'w') as f:\n",
    "    json.dump(deployment_instructions, f, indent=2)\n",
    "\n",
    "print(f\"✅ Deployment instructions saved: deployment_instructions.json\")\n",
    "\n",
    "print(f\"\\n📁 FINAL DEPLOYMENT PACKAGE:\")\n",
    "print(f\"├── 🤖 MODEL FILES (Choose one):\")\n",
    "print(f\"│   ├── best_pytorch_model.pkl     (Joblib pickle - RECOMMENDED)\")\n",
    "print(f\"│   ├── best_pytorch_model.pickle  (Standard pickle)\")\n",
    "print(f\"│   ├── best_pytorch_model.joblib  (Joblib format)\")\n",
    "print(f\"│   └── best_pytorch_model.pth     (PyTorch format)\")\n",
    "print(f\"├── 🔧 PREPROCESSING:\")\n",
    "print(f\"│   ├── feature_names.pkl          (35 feature names)\")\n",
    "print(f\"│   ├── scaler.pkl                 (StandardScaler)\")\n",
    "print(f\"│   └── feature_info.json          (Feature metadata)\")\n",
    "print(f\"├── 📝 DEPLOYMENT TOOLS:\")\n",
    "print(f\"│   ├── model_wrapper.py           (Easy-to-use wrapper class)\")\n",
    "print(f\"│   └── deployment_instructions.json (Complete usage guide)\")\n",
    "print(f\"└── 📊 DOCUMENTATION:\")\n",
    "print(f\"    └── deployment_summary.json    (Summary and metadata)\")\n",
    "\n",
    "print(f\"\\n🎯 RECOMMENDED USAGE:\")\n",
    "print(f\"   1. Use: best_pytorch_model.pkl (easiest to load)\")\n",
    "print(f\"   2. Use: model_wrapper.py (handles everything automatically)\")\n",
    "print(f\"   3. Example: model = OralHealthMLPWrapper('best_pytorch_model.pkl', 'scaler.pkl', 'feature_names.pkl')\")\n",
    "\n",
    "# Test the pickle model loading\n",
    "print(f\"\\n🧪 TESTING PICKLE MODEL LOADING:\")\n",
    "try:\n",
    "    test_model = joblib.load(pytorch_pkl_path)\n",
    "    print(f\"✅ Pickle model loads successfully\")\n",
    "    print(f\"   Model type: {type(test_model)}\")\n",
    "    print(f\"   Contains keys: {list(test_model.keys())}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading pickle model: {e}\")\n",
    "\n",
    "logger.info(\"Comprehensive deployment package created with multiple model formats!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
