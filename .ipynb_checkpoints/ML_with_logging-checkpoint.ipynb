{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ce73e07",
   "metadata": {},
   "source": [
    "# Enhanced MLP Architecture + Hyperparameter Exploration Notebook\n",
    "\n",
    "**What this notebook does**\n",
    "- Loads a CSV file that is already *categorically encoded* (user must provide path).\n",
    "- Tries a configurable range of MLP architectures and hyperparameters (hidden layers, solvers, batch sizes, learning rates, epochs, and — for PyTorch models — dropout).\n",
    "- Runs experiments using both **scikit-learn MLP** (fast, good for many grid search experiments) and an optional **PyTorch MLP** (to test dropout explicitly and obtain training loss/accuracy curves).\n",
    "- **Enhanced logging**: Comprehensive logging with TensorBoard, MLflow, progress tracking, error handling, and resource monitoring.\n",
    "- Saves several graphs (PNG files) showing how different parameters affect training/test performance and training loss curves.\n",
    "- Saves results to a CSV for later inspection.\n",
    "\n",
    "**Notes**\n",
    "- The notebook assumes the CSV includes features and a target column (default target column is `risk_level`). If your target has string labels, the notebook will attempt to map `low/medium/high` to `0/1/2`. Adjust the mapping if needed.\n",
    "- The notebook uses `StandardScaler` to scale inputs (recommended for neural nets).\n",
    "- The PyTorch implementation supports dropout and logs all metrics to TensorBoard and optionally MLflow.\n",
    "- Enhanced with comprehensive logging, error handling, and resource monitoring.\n",
    "\n",
    "You can run this notebook end-to-end; change the parameter grids near the top to expand or narrow the search.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "setup_logging",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced imports with logging and monitoring\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import product\n",
    "from datetime import datetime\n",
    "import logging\n",
    "import traceback\n",
    "import psutil\n",
    "import yaml\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ML imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Set up enhanced logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('experiment.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configuration\n",
    "OUTDIR = 'experiment_outputs'\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTDIR, 'models'), exist_ok=True)\n",
    "os.makedirs(os.path.join(OUTDIR, 'tb_logs'), exist_ok=True)\n",
    "\n",
    "# Experiment configuration\n",
    "use_pytorch = True  # Set to False to skip PyTorch experiments\n",
    "use_mlflow = False  # Set to True to enable MLflow logging\n",
    "data_path = '/mnt/data/sample.csv'  # Change this to your CSV path\n",
    "\n",
    "logger.info(f\"Starting ML experiments at {datetime.now()}\")\n",
    "logger.info(f\"Output directory: {OUTDIR}\")\n",
    "logger.info(f\"PyTorch enabled: {use_pytorch}\")\n",
    "logger.info(f\"MLflow enabled: {use_mlflow}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "data_loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "try:\n",
    "    logger.info(f\"Loading data from {data_path}\")\n",
    "    df = pd.read_csv(\"training_data.csv\")\n",
    "    logger.info(f\"Data loaded successfully. Shape: {df.shape}\")\n",
    "    logger.info(f\"Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Display basic info about the dataset\n",
    "    print(\"Dataset Info:\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    print(f\"Missing values: {df.isnull().sum().sum()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to load data: {str(e)}\")\n",
    "    logger.error(traceback.format_exc())\n",
    "    raise\n",
    "\n",
    "# Prepare features and target\n",
    "target_col = 'risk_level'  # Change this if your target column has a different name\n",
    "\n",
    "try:\n",
    "    if target_col not in df.columns:\n",
    "        logger.warning(f\"Target column '{target_col}' not found. Available columns: {list(df.columns)}\")\n",
    "        target_col = input(\"Please enter the correct target column name: \")\n",
    "    \n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Handle categorical target if needed\n",
    "    if y.dtype == 'object':\n",
    "        logger.info(f\"Converting categorical target. Unique values: {y.unique()}\")\n",
    "        if set(y.unique()).issubset({'low', 'medium', 'high'}):\n",
    "            y = y.map({'low': 0, 'medium': 1, 'high': 2})\n",
    "            logger.info(\"Mapped risk levels: low->0, medium->1, high->2\")\n",
    "        else:\n",
    "            # Use label encoding for other categorical targets\n",
    "            from sklearn.preprocessing import LabelEncoder\n",
    "            le = LabelEncoder()\n",
    "            y = le.fit_transform(y)\n",
    "            logger.info(f\"Label encoded target. Classes: {le.classes_}\")\n",
    "    \n",
    "    logger.info(f\"Target distribution: {np.bincount(y)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Error preparing target variable: {str(e)}\")\n",
    "    logger.error(traceback.format_exc())\n",
    "    raise\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "logger.info(f\"Train-test split completed. Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_s = scaler.fit_transform(X_train)\n",
    "X_test_s = scaler.transform(X_test)\n",
    "logger.info(\"Features scaled using StandardScaler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experiment_config",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grids\n",
    "sklearn_param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50), (100, 100)],\n",
    "    'solver': ['adam', 'lbfgs'],\n",
    "    'learning_rate_init': [0.001, 0.01, 0.1],\n",
    "    'max_iter': [200, 500]\n",
    "}\n",
    "\n",
    "pytorch_param_grid = {\n",
    "    'hidden_sizes': [[64], [128], [64, 32], [128, 64], [128, 64, 32]],\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "    'dropout': [0.0, 0.2, 0.5],\n",
    "    'batch_size': [32, 64],\n",
    "    'epochs': [50, 100, 150]\n",
    "}\n",
    "\n",
    "# Save experiment configuration\n",
    "experiment_config = {\n",
    "    'sklearn_param_grid': sklearn_param_grid,\n",
    "    'pytorch_param_grid': pytorch_param_grid,\n",
    "    'data_path': data_path,\n",
    "    'target_column': target_col,\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'train_size': X_train.shape[0],\n",
    "    'test_size': X_test.shape[0],\n",
    "    'n_features': X_train.shape[1],\n",
    "    'n_classes': len(np.unique(y)),\n",
    "    'use_pytorch': use_pytorch,\n",
    "    'use_mlflow': use_mlflow\n",
    "}\n",
    "\n",
    "with open(os.path.join(OUTDIR, 'experiment_config.yaml'), 'w') as f:\n",
    "    yaml.dump(experiment_config, f, default_flow_style=False)\n",
    "\n",
    "logger.info(f\"Experiment configuration saved to {os.path.join(OUTDIR, 'experiment_config.yaml')}\")\n",
    "print(f\"Total sklearn combinations: {len(list(product(*sklearn_param_grid.values())))}\")\n",
    "if use_pytorch:\n",
    "    print(f\"Total PyTorch combinations: {len(list(product(*pytorch_param_grid.values())))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sklearn_experiments",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn MLP experiments\n",
    "sklearn_results = []\n",
    "combos = list(product(*sklearn_param_grid.values()))\n",
    "keys = list(sklearn_param_grid.keys())\n",
    "\n",
    "logger.info(f\"Starting {len(combos)} scikit-learn experiments\")\n",
    "\n",
    "for i, combo in enumerate(tqdm(combos, desc=\"Scikit-learn experiments\")):\n",
    "    try:\n",
    "        params = dict(zip(keys, combo))\n",
    "        logger.debug(f\"Testing sklearn params: {params}\")\n",
    "        \n",
    "        # Log system resources\n",
    "        memory_usage = psutil.virtual_memory().percent\n",
    "        cpu_usage = psutil.cpu_percent(interval=1)\n",
    "        \n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        clf = MLPClassifier(**params, random_state=42)\n",
    "        clf.fit(X_train_s, y_train)\n",
    "        \n",
    "        train_pred = clf.predict(X_train_s)\n",
    "        test_pred = clf.predict(X_test_s)\n",
    "        train_acc = accuracy_score(y_train, train_pred)\n",
    "        test_acc = accuracy_score(y_test, test_pred)\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        duration = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        result = {\n",
    "            'experiment_id': i + 1,\n",
    "            'params': params,\n",
    "            'train_acc': train_acc,\n",
    "            'test_acc': test_acc,\n",
    "            'n_iter': clf.n_iter_,\n",
    "            'loss_curve': clf.loss_curve_ if hasattr(clf, 'loss_curve_') else None,\n",
    "            'duration_seconds': duration,\n",
    "            'memory_usage_percent': memory_usage,\n",
    "            'cpu_usage_percent': cpu_usage,\n",
    "            'timestamp': start_time.isoformat()\n",
    "        }\n",
    "        \n",
    "        sklearn_results.append(result)\n",
    "        \n",
    "        logger.info(f\"Sklearn experiment {i+1}/{len(combos)} completed. Train acc: {train_acc:.4f}, Test acc: {test_acc:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Sklearn experiment {i+1} failed with params {params}: {str(e)}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        continue\n",
    "\n",
    "logger.info(f\"Completed {len(sklearn_results)} successful scikit-learn experiments\")\n",
    "\n",
    "# Save sklearn results\n",
    "if sklearn_results:\n",
    "    sklearn_df = pd.DataFrame([\n",
    "        {\n",
    "            'experiment_id': r['experiment_id'],\n",
    "            'hidden_layer_sizes': str(r['params']['hidden_layer_sizes']),\n",
    "            'solver': r['params']['solver'],\n",
    "            'learning_rate_init': r['params']['learning_rate_init'],\n",
    "            'max_iter': r['params']['max_iter'],\n",
    "            'train_acc': r['train_acc'],\n",
    "            'test_acc': r['test_acc'],\n",
    "            'n_iter': r['n_iter'],\n",
    "            'duration_seconds': r['duration_seconds'],\n",
    "            'memory_usage_percent': r['memory_usage_percent'],\n",
    "            'cpu_usage_percent': r['cpu_usage_percent'],\n",
    "            'timestamp': r['timestamp']\n",
    "        } for r in sklearn_results\n",
    "    ])\n",
    "    sklearn_df.to_csv(os.path.join(OUTDIR, 'sklearn_results_summary.csv'), index=False)\n",
    "    logger.info(\"Scikit-learn results saved to sklearn_results_summary.csv\")\n",
    "    \n",
    "    # Display best results\n",
    "    best_sklearn = sklearn_df.loc[sklearn_df['test_acc'].idxmax()]\n",
    "    print(f\"\\nBest scikit-learn result:\")\n",
    "    print(f\"Test accuracy: {best_sklearn['test_acc']:.4f}\")\n",
    "    print(f\"Parameters: hidden_sizes={best_sklearn['hidden_layer_sizes']}, solver={best_sklearn['solver']}, lr={best_sklearn['learning_rate_init']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pytorch_setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch experiments setup\n",
    "if use_pytorch:\n",
    "    try:\n",
    "        import torch\n",
    "        import torch.nn as nn\n",
    "        import torch.optim as optim\n",
    "        from torch.utils.data import TensorDataset, DataLoader\n",
    "        from torch.utils.tensorboard import SummaryWriter\n",
    "        \n",
    "        # MLflow setup (optional)\n",
    "        if use_mlflow:\n",
    "            import mlflow\n",
    "            import mlflow.pytorch\n",
    "            mlflow.set_experiment(\"MLP_Hyperparameter_Search\")\n",
    "            logger.info(\"MLflow experiment tracking enabled\")\n",
    "        \n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        logger.info(f'PyTorch device: {device}')\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            logger.info(f'CUDA device: {torch.cuda.get_device_name(0)}')\n",
    "            logger.info(f'CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB')\n",
    "        \n",
    "        def build_pytorch_model(input_size, hidden_sizes, dropout=0.0, n_classes=None):\n",
    "            layers = []\n",
    "            in_size = input_size\n",
    "            for h in hidden_sizes:\n",
    "                layers.append(nn.Linear(in_size, h))\n",
    "                layers.append(nn.ReLU())\n",
    "                if dropout and dropout > 0.0:\n",
    "                    layers.append(nn.Dropout(dropout))\n",
    "                in_size = h\n",
    "            layers.append(nn.Linear(in_size, n_classes))\n",
    "            return nn.Sequential(*layers)\n",
    "        \n",
    "        # Prepare PyTorch tensors\n",
    "        X_train_t = torch.FloatTensor(X_train_s)\n",
    "        y_train_t = torch.LongTensor(y_train.values if hasattr(y_train, 'values') else y_train)\n",
    "        X_test_t = torch.FloatTensor(X_test_s)\n",
    "        y_test_t = torch.LongTensor(y_test.values if hasattr(y_test, 'values') else y_test)\n",
    "        \n",
    "        logger.info(\"PyTorch setup completed successfully\")\n",
    "        \n",
    "    except ImportError as e:\n",
    "        logger.error(f\"PyTorch import failed: {str(e)}\")\n",
    "        use_pytorch = False\n",
    "    except Exception as e:\n",
    "        logger.error(f\"PyTorch setup failed: {str(e)}\")\n",
    "        logger.error(traceback.format_exc())\n",
    "        use_pytorch = False\n",
    "else:\n",
    "    logger.info(\"PyTorch experiments disabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pytorch_experiments",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced PyTorch experiments\n",
    "if use_pytorch:\n",
    "    pytorch_results = []\n",
    "    combos = list(product(*pytorch_param_grid.values()))\n",
    "    keys = list(pytorch_param_grid.keys())\n",
    "    \n",
    "    logger.info(f\"Starting {len(combos)} PyTorch experiments\")\n",
    "    \n",
    "    run_counter = 0\n",
    "    best_test_acc = 0.0\n",
    "    \n",
    "    for combo in tqdm(combos, desc=\"PyTorch experiments\"):\n",
    "        run_counter += 1\n",
    "        \n",
    "        try:\n",
    "            # Setup logging\n",
    "            log_dir = os.path.join(OUTDIR, f'tb_logs/run_{run_counter}')\n",
    "            writer = SummaryWriter(log_dir=log_dir)\n",
    "            \n",
    "            params = dict(zip(keys, combo))\n",
    "            logger.info(f\"\\nPyTorch experiment {run_counter}/{len(combos)}: {params}\")\n",
    "            \n",
    "            # MLflow run start\n",
    "            if use_mlflow:\n",
    "                mlflow.start_run()\n",
    "                mlflow.log_params(params)\n",
    "            \n",
    "            # Model setup\n",
    "            input_size = X_train_s.shape[1]\n",
    "            n_classes = len(np.unique(y_train))\n",
    "            model = build_pytorch_model(\n",
    "                input_size, \n",
    "                params['hidden_sizes'], \n",
    "                dropout=params['dropout'], \n",
    "                n_classes=n_classes\n",
    "            ).to(device)\n",
    "            \n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=params['learning_rate'])\n",
    "            \n",
    "            # Data loader\n",
    "            train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "            train_loader = DataLoader(train_ds, batch_size=params['batch_size'], shuffle=True)\n",
    "            \n",
    "            # Training tracking\n",
    "            train_losses = []\n",
    "            val_losses = []\n",
    "            val_accuracies = []\n",
    "            start_time = datetime.now()\n",
    "            \n",
    "            # Training loop with enhanced logging\n",
    "            for epoch in range(params['epochs']):\n",
    "                model.train()\n",
    "                epoch_losses = []\n",
    "                \n",
    "                for batch_idx, (xb, yb) in enumerate(train_loader):\n",
    "                    xb = xb.to(device)\n",
    "                    yb = yb.to(device)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    out = model(xb)\n",
    "                    loss = criterion(out, yb)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    epoch_losses.append(loss.item())\n",
    "                \n",
    "                avg_train_loss = np.mean(epoch_losses)\n",
    "                train_losses.append(avg_train_loss)\n",
    "                \n",
    "                # Validation every 5 epochs\n",
    "                if epoch % 5 == 0 or epoch == params['epochs'] - 1:\n",
    "                    model.eval()\n",
    "                    with torch.no_grad():\n",
    "                        val_out = model(X_test_t.to(device))\n",
    "                        val_loss = criterion(val_out, y_test_t.to(device))\n",
    "                        val_pred = val_out.argmax(dim=1).cpu().numpy()\n",
    "                        val_acc = accuracy_score(y_test, val_pred)\n",
    "                    \n",
    "                    val_losses.append(val_loss.item())\n",
    "                    val_accuracies.append(val_acc)\n",
    "                    \n",
    "                    # TensorBoard logging\n",
    "                    writer.add_scalar('Loss/train', avg_train_loss, epoch)\n",
    "                    writer.add_scalar('Loss/validation', val_loss.item(), epoch)\n",
    "                    writer.add_scalar('Accuracy/validation', val_acc, epoch)\n",
    "                    \n",
    "                    # MLflow logging\n",
    "                    if use_mlflow:\n",
    "                        mlflow.log_metric(\"train_loss\", avg_train_loss, step=epoch)\n",
    "                        mlflow.log_metric(\"val_loss\", val_loss.item(), step=epoch)\n",
    "                        mlflow.log_metric(\"val_accuracy\", val_acc, step=epoch)\n",
    "                    \n",
    "                    # System resource monitoring\n",
    "                    memory_usage = psutil.virtual_memory().percent\n",
    "                    if torch.cuda.is_available():\n",
    "                        gpu_memory = torch.cuda.memory_allocated() / 1024**3\n",
    "                        writer.add_scalar('System/gpu_memory_gb', gpu_memory, epoch)\n",
    "                    writer.add_scalar('System/memory_usage_percent', memory_usage, epoch)\n",
    "                \n",
    "                # Progress reporting\n",
    "                if (epoch + 1) % 20 == 0 or epoch == 0:\n",
    "                    logger.debug(f\"Epoch {epoch+1}/{params['epochs']}: train_loss={avg_train_loss:.4f}\")\n",
    "            \n",
    "            # Final evaluation\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                out_train = model(X_train_t.to(device))\n",
    "                pred_train = out_train.argmax(dim=1).cpu().numpy()\n",
    "                out_test = model(X_test_t.to(device))\n",
    "                pred_test = out_test.argmax(dim=1).cpu().numpy()\n",
    "            \n",
    "            train_acc = accuracy_score(y_train, pred_train)\n",
    "            test_acc = accuracy_score(y_test, pred_test)\n",
    "            \n",
    "            end_time = datetime.now()\n",
    "            duration = (end_time - start_time).total_seconds()\n",
    "            \n",
    "            # Confusion matrix logging\n",
    "            cm = confusion_matrix(y_test, pred_test)\n",
    "            fig, ax = plt.subplots(figsize=(8, 6))\n",
    "            sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap='Blues')\n",
    "            ax.set_title(f'Confusion Matrix - Run {run_counter}')\n",
    "            ax.set_xlabel('Predicted')\n",
    "            ax.set_ylabel('Actual')\n",
    "            writer.add_figure('Confusion_Matrix', fig, global_step=run_counter)\n",
    "            plt.close(fig)\n",
    "            \n",
    "            # Training loss curve plot\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "            \n",
    "            ax1.plot(train_losses, label='Training Loss')\n",
    "            if val_losses:\n",
    "                val_epochs = list(range(0, params['epochs'], 5)) + [params['epochs'] - 1]\n",
    "                ax1.plot(val_epochs[:len(val_losses)], val_losses, label='Validation Loss', marker='o')\n",
    "            ax1.set_title(f\"Loss Curves - Run {run_counter}\")\n",
    "            ax1.set_xlabel('Epoch')\n",
    "            ax1.set_ylabel('Loss')\n",
    "            ax1.legend()\n",
    "            ax1.grid(True)\n",
    "            \n",
    "            if val_accuracies:\n",
    "                ax2.plot(val_epochs[:len(val_accuracies)], val_accuracies, label='Validation Accuracy', marker='o', color='green')\n",
    "                ax2.set_title(f\"Validation Accuracy - Run {run_counter}\")\n",
    "                ax2.set_xlabel('Epoch')\n",
    "                ax2.set_ylabel('Accuracy')\n",
    "                ax2.legend()\n",
    "                ax2.grid(True)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save plot\n",
    "            fname = f\"pytorch_metrics_run_{run_counter}.png\"\n",
    "            plt.savefig(os.path.join(OUTDIR, fname), dpi=300, bbox_inches='tight')\n",
    "            plt.close()\n",
    "            \n",
    "            # Model checkpointing for best model\n",
    "            if test_acc > best_test_acc:\n",
    "                best_test_acc = test_acc\n",
    "                checkpoint = {\n",
    "                    'model_state_dict': model.state_dict(),\n",
    "                    'params': params\n",
    "                }\n",
    "                torch.save(checkpoint, os.path.join(OUTDIR, 'models', 'best_model.pth'))\n",
    "                logger.info(f\"New best model saved with test accuracy: {test_acc:.4f}\")\n",
    "            \n",
    "            result = {\n",
    "                'experiment_id': run_counter,\n",
    "                'params': params,\n",
    "                'train_acc': train_acc,\n",
    "                'test_acc': test_acc,\n",
    "                'duration_seconds': duration,\n",
    "                'final_train_loss': train_losses[-1] if train_losses else None,\n",
    "                'final_val_loss': val_losses[-1] if val_losses else None,\n",
    "                'final_val_accuracy': val_accuracies[-1] if val_accuracies else None,\n",
    "                'timestamp': start_time.isoformat()\n",
    "            }\n",
    "            \n",
    "            pytorch_results.append(result)\n",
    "            \n",
    "            logger.info(f\"PyTorch experiment {run_counter} completed. Train acc: {train_acc:.4f}, Test acc: {test_acc:.4f}\")\n",
    "            \n",
    "            if use_mlflow:\n",
    "                mlflow.log_artifact(os.path.join(OUTDIR, fname), \"plots\")\n",
    "                mlflow.log_metric(\"final_test_accuracy\", test_acc)\n",
    "                mlflow.log_metric(\"duration\", duration)\n",
    "                mlflow.end_run()\n",
    "            \n",
    "            writer.close()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"PyTorch experiment {run_counter} failed with params {params}: {str(e)}\")\n",
    "            logger.error(traceback.format_exc())\n",
    "            if use_mlflow:\n",
    "                mlflow.end_run(status=\"FAILED\")\n",
    "            continue\n",
    "    \n",
    "    logger.info(f\"Completed {len(pytorch_results)} successful PyTorch experiments\")\n",
    "    \n",
    "    # Save PyTorch results\n",
    "    if pytorch_results:\n",
    "        pytorch_df = pd.DataFrame([\n",
    "            {\n",
    "                'experiment_id': r['experiment_id'],\n",
    "                'hidden_sizes': str(r['params']['hidden_sizes']),\n",
    "                'learning_rate': r['params']['learning_rate'],\n",
    "                'dropout': r['params']['dropout'],\n",
    "                'batch_size': r['params']['batch_size'],\n",
    "                'epochs': r['params']['epochs'],\n",
    "                'train_acc': r['train_acc'],\n",
    "                'test_acc': r['test_acc'],\n",
    "                'duration_seconds': r['duration_seconds'],\n",
    "                'final_train_loss': r['final_train_loss'],\n",
    "                'final_val_loss': r['final_val_loss'],\n",
    "                'final_val_accuracy': r['final_val_accuracy'],\n",
    "                'timestamp': r['timestamp']\n",
    "            } for r in pytorch_results\n",
    "        ])\n",
    "        pytorch_df.to_csv(os.path.join(OUTDIR, 'pytorch_results_summary.csv'), index=False)\n",
    "        logger.info(\"PyTorch results saved to pytorch_results_summary.csv\")\n",
    "        \n",
    "        # Display best results\n",
    "        best_pytorch = pytorch_df.loc[pytorch_df['test_acc'].idxmax()]\n",
    "        print(f\"\\nBest PyTorch result:\")\n",
    "        print(f\"Test accuracy: {best_pytorch['test_acc']:.4f}\")\n",
    "        print(f\"Parameters: hidden_sizes={best_pytorch['hidden_sizes']}, lr={best_pytorch['learning_rate']}, dropout={best_pytorch['dropout']}\")\n",
    "    else:\n",
    "        logger.warning(\"No successful PyTorch experiments to save\")\n",
    "else:\n",
    "    logger.info(\"Skipping PyTorch experiments as requested\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plotting_results",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting and final analysis\n",
    "logger.info(\"Starting final analysis and plotting of results\")\n",
    "\n",
    "def create_performance_plots(df, metric_col, plot_title, filename):\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Convert string representations of tuples/lists to a usable format for plotting\n",
    "    # This is a bit of a hack but works for this specific case\n",
    "    if 'hidden_layer_sizes' in df.columns:\n",
    "        df['hidden_layer_sizes_str'] = df['hidden_layer_sizes'].apply(lambda x: str(x).replace(' ', ''))\n",
    "        x_col = 'hidden_layer_sizes_str'\n",
    "        hue_col = 'solver'\n",
    "    else:\n",
    "        df['hidden_sizes_str'] = df['hidden_sizes'].apply(lambda x: str(x).replace(' ', ''))\n",
    "        x_col = 'hidden_sizes_str'\n",
    "        hue_col = 'dropout'\n",
    "        \n",
    "    sns.barplot(x=x_col, y=metric_col, hue=hue_col, data=df, ax=ax, palette='viridis')\n",
    "    ax.set_title(plot_title)\n",
    "    ax.set_xlabel('Architecture')\n",
    "    ax.set_ylabel(metric_col)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "    ax.grid(axis='y', linestyle='--')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUTDIR, filename), dpi=300)\n",
    "    plt.close()\n",
    "    logger.info(f\"Saved plot: {filename}\")\n",
    "\n",
    "if 'sklearn_df' in locals() and not sklearn_df.empty:\n",
    "    create_performance_plots(sklearn_df, 'test_acc', 'Scikit-learn MLP Test Accuracy by Architecture and Solver', 'sklearn_test_accuracy.png')\n",
    "    \n",
    "if 'pytorch_df' in locals() and not pytorch_df.empty:\n",
    "    create_performance_plots(pytorch_df, 'test_acc', 'PyTorch MLP Test Accuracy by Architecture and Dropout', 'pytorch_test_accuracy.png')\n",
    "\n",
    "logger.info(\"Final analysis and plotting complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv_oralsmart_data_analysis)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
